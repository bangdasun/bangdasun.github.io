<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (3) - Regression Tree | Machine Learning | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content=",">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (3) - Regression Tree"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (3) - Regression Tree</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/posts/ml-overview-regression-tree.html" rel="bookmark">
        <time class="entry-date published" datetime="2017-09-04T00:11:58.000Z">
          2017-09-03
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Easy to interpret but tricks are needed in implementation.</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<a id="more"></a>
<p>If the mathematical formula parts just show the TeX code, see this <a href="https://bangdasun.github.io/2017/08/22/about-mathematics-formula-display/" target="_blank" rel="external">post</a>.</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Decision tree is a family of algorithm which are widely used in machine learning. It is not only an algorithm for itself, but also be the basic unit of ensemble algorithms like random forest and boosting.</p>
<p>Decision tree is easy to interpret since it’s structure is similar to the decision process of human, and if you have basic experience working with data structures you’ll see that the structure is identical with binary tree. For example when the hiring manager is viewing resumes for applying data scientist position, the general procedure might be like this: If the applicants have related working experience? next, if the applicants’ skill sets match with the job description? next, if the applicants have degree in mathematics/statistics/computer science, so on so forth.</p>
<p>Decision tree could be used to tackle both classification and regression problems, a.k.a Classification and Regression Tree (<strong>CART</strong>). In this post I will briefly go through the mechanism and implementation of regression tree.</p>
<h3 id="Mathematical-Basis"><a href="#Mathematical-Basis" class="headerlink" title="Mathematical Basis"></a>Mathematical Basis</h3><p>We first define several concepts for decision trees. A tree consists of <strong>nodes</strong>, <strong>leaves</strong> and the linkage among them (node to node, node to leaf). The top node is called <strong>root</strong>, and the last level of the tree would be leaves. In each node a decision is made based on one rule, expressed as \(\{X|X_{j}&lt;s\}\), which means the data set will split into two parts (left child and right child from the view of tree), observations with \(j\) th feature less than \(s\) will go to left child node otherwise it will go to right child node. Then we do same manipulation on left child node and right child node.</p>
<p>You can see that the key part is <strong>split</strong>, actually we are dividing the feature space into small blocks as we splitting features. Then we just go straightforward: we would like the split process (building process of the tree) could minimize error in prediction, equivalently we look for <strong>blocks</strong> sequence \(B_{1}, B_{2},\cdots, B_{n}\) that could minimize error.</p>
<p>Since this is a regression problem, we use the <strong>mean value of all observations in one block as the prediction</strong> of observations fall into that block, and we can choose squared loss / absolute loss, etc. Here I will use squared loss as an example. Now like many other machine learning algorithms, it is an optimization problem,</p>
<p>\[<br>B_{1}, B_{2},\cdots, B_{n} = \arg\min_{B_{1}, B_{2},\cdots, B_{n}}\sum^{n}_{i=1}\sum_{j: x_{j}\in B_{i}}\left(y_{j}-\bar{y}_{B_{i}}\right)^{2}.<br>\]</p>
<p>Looks perfect however …… those scientists said that this problem is computationally infeasible to consider every possible partition of the feature space into \(n\) blocks…:-(</p>
<p>But those awesome scientists found an alternative way, they came up with an idea called <strong>top-down greedy recursive binary splitting</strong>, which means it begins at the top of the tree where all observations are still in one block, then continuously split the feature space where each split will generate a left child node and right child node. Each step an optimize split is selected therefore it’s “greedy”. Therefore instead of find a global optima, we turn to find a local optima (global optima in every stage). The problem would looks like this: find a split \(\{X |X_{j} &lt; s\}\), where this split generate \(B_{1} = \{X|X_{j} &lt; s\}\) and \(B_{2} = \{X|X_{j} \geq s\}\) that </p>
<p>\[<br>(j, s) = \arg\min_{(j,s)}\sum_{i:x_{i}\in B_{1}}\left(y_{i}-\bar{y}_{B_{1}}\right)^{2} + \sum_{j:x_{j}\in B_{2}}\left(y_{j}-\bar{y}_{B_{2}}\right)^{2}.<br>\]</p>
<p>Then we repeat this process on data in \(B_{1}\) and \(B_{2}\) and so on so forth. When do we stop? we can specify the maximum depth <code>max_depth</code> (the longest step length from root to the leaves) and minimum number of observations needed to make a split <code>min_split_sample</code>.</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>The implementation of decision tree could be tricky, since you don’t know how many splits needed exactly, also it’s dynamic process since every time a new split will be applied on a new block. </p>
<p>First I used <code>R</code> and quickly found that my R-coding is still not good enough to finish this job, I can only perform every split manually…here is my <a href="https://github.com/bangdasun/Statistical-machine-learning/blob/master/algorithms/regression/regression-decision-tree/self-defined-recursive-manually.R" target="_blank" rel="external">solutions</a> and you can totally ignore this one :-)</p>
<p>Then I turned to <code>Python</code> for help. I defined classes called <code>Node</code> and <code>RegressionTree</code>, and defined a recursive method to perform the split, which is better and convenient <a href="https://github.com/bangdasun/Statistical-machine-learning/blob/master/algorithms/regression/regression-decision-tree/self-defined.py" target="_blank" rel="external">Python solution</a>. Here is a <a href="https://github.com/bangdasun/Statistical-machine-learning/blob/master/algorithms/regression/regression-decision-tree/self-defined.ipynb" target="_blank" rel="external">better solution</a> I found without using OOP, I did little revision to make it works in my situation and it out-performed than my solution +_+…From this practice I clearly realized that I need more programming practice…</p>
<h3 id="More-Discussion"><a href="#More-Discussion" class="headerlink" title="More Discussion"></a>More Discussion</h3><p>In practice, we will apply the decision on new data, therefore how to control overfitting would be an issue. It’s easy to make a complex tree that might has poor performance on test data. Based on bias-variance trade-off principal, we would like to sacrifice some bias to reduce variance. One of strategies is pruning the tree - grow a large tree first then prune it back to a smaller subtree. In addition we can control <code>max_depth</code> and <code>min_split_sample</code>. I will get to that point in later posts.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/undefined">undefined</a><a href="/undefined">undefined</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2020 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>