<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (3) - Regression Tree | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Decision Tree,Regression">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (3) - Regression Tree"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (3) - Regression Tree</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/09/03/8-ml-overview-3-regression-tree/" rel="bookmark">
        <time class="entry-date published" datetime="2017-09-04T01:11:58.000Z">
          2017-09-03
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Easy to interpret but tricks are needed in implementation</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Decision tree is a family of algorithm which are widely used in machine learning. It is not only an algorithm by itself, but also the basic unit of ensemble algorithms like random forest and boosting.</p>
<p>Decision tree is easy to interpret since it’s structure is similar to the decision process of human, and if you have data structures basis you’ll see that the structure is exactly <strong>binary tree</strong>. For example when the hiring manager is viewing resumes for applying data scientist position, the general procedure might be like this: If the applicants have related working experience? next, if the applicants’ skill sets match with the job description? next, if the applicants have degree in mathematics/statistics/computer science, so on so forth.</p>
<p>There are many decision tree algorithm, including:</p>
<ul>
<li>ID3 (Iterative Dichotomiser 3)</li>
<li>C4.5 (successor of ID3)</li>
<li>CART (Classification And Regression Tree)</li>
</ul>
<p>the differences are mainly about learning algorithm (such as loss function). <strong>CART</strong> can be used to tackle both classification and regression problems, they are <strong>non-parametric</strong> method since no assumptions are made on the data. In this post I will briefly go through the mechanism and implementation of regression tree.</p>
<h3 id="2-Mathematical-Basis"><a href="#2-Mathematical-Basis" class="headerlink" title="2. Mathematical Basis"></a>2. Mathematical Basis</h3><h4 id="2-1-Basic-Elements-of-Decision-Tree"><a href="#2-1-Basic-Elements-of-Decision-Tree" class="headerlink" title="2.1 Basic Elements of Decision Tree"></a>2.1 Basic Elements of Decision Tree</h4><p>We first define several concepts for decision trees. A tree consists of <strong>nodes</strong>, <strong>leaves</strong> and the linkage among them (node to node, node to leaf). The top node is called <strong>root</strong>, and the last level of the tree would be leaves. In each node a decision is made based on one rule, expressed as \(\{X|X_{j}&lt;s\}\), which means the data set will split into two parts (left child and right child from the view of tree), observations with \(j\) th feature less than \(s\) will go to left child node otherwise it will go to right child node. Then we do same manipulation on left child node and right child node.</p>
<h4 id="2-2-Split-Finding"><a href="#2-2-Split-Finding" class="headerlink" title="2.2. Split Finding"></a>2.2. Split Finding</h4><p>The key part of learning decision tree is <strong>split</strong>, i.e. find the value of a specific feature to divide the feature space into small blocks. Then we just go straightforward: we would like the split process (building process of the tree) could minimize error in prediction, equivalently we look for <strong>blocks</strong> sequence \(B_{1}, B_{2},\cdots, B_{n}\) that could minimize error.</p>
<p>Since this is a regression problem, we use the <strong>mean value of all observations in one block as the prediction</strong> of observations fall into that block, and we can choose squared loss / absolute loss, etc. Here I will use squared loss as an example. Now like many other machine learning algorithms, it is an optimization problem, i.e. reduce the <strong>MSE (squared loss)</strong> by splitting,</p>
<p>\[<br>B_{1}, B_{2},\cdots, B_{n} = \arg\min_{B_{1}, B_{2},\cdots, B_{n}}\sum^{n}_{i=1}\sum_{j: x_{j}\in B_{i}}\left(y_{j}-\bar{y}_{B_{i}}\right)^{2}.<br>\]</p>
<p>Unfortunately this problem is computationally infeasible to consider every possible partition of the feature space into \(n\) blocks.</p>
<p>But those awesome scientists found an alternative way, they came up with an idea called <strong>top-down greedy recursive binary splitting</strong>, which means it begins at the top of the tree where all observations are still in one block, then continuously split the feature space where each split will generate a left child node and right child node. Each step an optimize split is selected therefore it’s “greedy”. Therefore instead of find a global optima, we turn to find a local optima (global optima in every stage). The problem would looks like this: find a split \(\{X |X_{j} &lt; s\}\), where this split generate \(B_{1} = \{X|X_{j} &lt; s\}\) and \(B_{2} = \{X|X_{j} \geq s\}\) such that </p>
<p>\[<br>(j, s) = \arg\min_{(j,s)}\sum_{i:x_{i}\in B_{1}}\left(y_{i}-\bar{y}_{B_{1}}\right)^{2} + \sum_{j:x_{j}\in B_{2}}\left(y_{j}-\bar{y}_{B_{2}}\right)^{2}.<br>\]</p>
<p>Then we repeat this process on data in \(B_{1}\) and \(B_{2}\) and go to the next level of the tree, it is a <strong>depth wise growth</strong>, kind of similar to <strong>BFS (Breadth First Search)</strong>. </p>
<p>The stop condition for growing tree could be set by specifying the maximum depth <code>max_depth</code> (the longest step length from root to the leaves) and minimum number of observations needed to make a split <code>min_split_sample</code>, or say the number of observations need in a leaf.</p>
<h3 id="3-Tree-Pruning"><a href="#3-Tree-Pruning" class="headerlink" title="3. Tree Pruning"></a>3. Tree Pruning</h3><p>It’s easy to make a complex tree that might has poor performance on test data - the extreme case could be . Based on bias-variance trade-off principal, we would like to <strong>sacrifice some bias to reduce variance</strong>. One of strategies is <strong>pruning</strong> the tree - grow a large tree first then prune it back to a smaller subtree. </p>
<p>One of the methods is <strong>Cost Complexity Pruning</strong>. This is <strong>down to top</strong> process. During the tree growing, a series of trees are generated \(T_{0}, T_{1}, \cdots, T_{m}\), where \(T_{m}\) is the root. At step \(i\), the tree is created by removing a subtree from tree \(i-1\) and replacing it with a leaf node.</p>
<p>In addition we can control <code>max_depth</code> and <code>min_split_sample</code>, or add tree size  \(|T|\) as a <strong>regularization</strong> term in the loss function.</p>
<h3 id="4-Implementation"><a href="#4-Implementation" class="headerlink" title="4. Implementation"></a>4. Implementation</h3><p>The implementation of decision tree could be tricky, since you don’t know how many splits needed exactly, also it’s dynamic process since every time a new split will be applied on a new block. </p>
<p>First I used <code>R</code> and quickly found that my R-coding is still not good enough to finish this job, I can only perform every split manually…here is my <a href="https://github.com/bangdasun/Statistical-machine-learning/blob/master/algorithms/regression/regression-decision-tree/self-defined-recursive-manually.R" target="_blank" rel="external">solutions</a> and you can totally ignore this one :-)</p>
<p>Then I turned to <code>Python</code> for help. I defined classes called <code>Node</code> and <code>RegressionTree</code>, and defined a recursive method to perform the split, which is better and convenient <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/regression/regression-decision-tree/CART-R.py" target="_blank" rel="external">Python solution</a>. Here is a <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/regression/regression-decision-tree/CART-R.ipynb" target="_blank" rel="external">better solution</a> I found without using OOP, I did little revision to make it works in my situation and it out-performed than my solution… From this practice I clearly realized that I need more programming practice…</p>
<h3 id="5-References"><a href="#5-References" class="headerlink" title="5. References"></a>5. References</h3><ul>
<li>Gareth James, Daniela Witten, Trevor Hastie, Robert Tishirani, Introduction to Statistical Learning.</li>
<li>Decision Tree, <a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank" rel="external">https://en.wikipedia.org/wiki/Decision_tree</a></li>
<li>Decision Tree Learning, <a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="external">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
<li>Decision Tree Pruning, <a href="https://en.wikipedia.org/wiki/Decision_tree_pruning" target="_blank" rel="external">https://en.wikipedia.org/wiki/Decision_tree_pruning</a></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/Regression/">Regression</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>