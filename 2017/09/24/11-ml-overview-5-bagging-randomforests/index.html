<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (5) - Bagging and Random Forests | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Classification,Ensemble Algorithms,Decision Tree">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (5) - Bagging and Random Forests"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (5) - Bagging and Random Forests</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/09/24/11-ml-overview-5-bagging-randomforests/" rel="bookmark">
        <time class="entry-date published" datetime="2017-09-25T00:19:01.000Z">
          2017-09-24
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Let’s start working with ensemble algorithms</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>After decision tree section we can go deeper. Bagging and Random Forests can be also regarded as an implementation of <strong>Collective Intelligence</strong>. Which means rather than single model, now we get many models in our hand. The names of these two algorithms are intuitive: for “Bagging”, image that we have a “bag” of decision trees and let them work together (actually “bagging” is for “bootstrap aggregation”); for “random forests” we have similar description, we are now facing “forests” rather than single tree. </p>
<h3 id="2-Mathematical-Basis"><a href="#2-Mathematical-Basis" class="headerlink" title="2. Mathematical Basis"></a>2. Mathematical Basis</h3><h4 id="2-1-Bootstrap-and-Bagging"><a href="#2-1-Bootstrap-and-Bagging" class="headerlink" title="2.1 Bootstrap and Bagging"></a>2.1 Bootstrap and Bagging</h4><p>First let’s introduce <strong>bootstrap</strong>, it is the base of this two algorithms. Bootstrap is a widely used method in statistics and other fields and it related to “Resampling”. One of the example is <strong>parameter estimation</strong>. Consider we have a random sample and we would like to estimate the population mean.  We usually use sample mean as the estimation of population mean since this is the “MLE”. But what if we want to know how confidence are we to this estimation? Well we might assume that the sample mean has Normal distribution (Do you remember Central Limit Theorem?) and do some inference; or we can refer Bayesian statisticians for help, set up a prior and we observe the data and then simulate the posterior. And we can also use bootstrap. Here is the idea:</p>
<blockquote>
<ol>
<li>We sample our data <strong>with replacement</strong> and get a new sample with same sample size, therefore it’s possible that there are duplicates in new sample;</li>
<li>Repeat this process multiple times, say \(B\). And finally we will have \(B\) samples;</li>
<li>Get all sample means from these \(B\) samples, now we can form a distribution and do what we want to do (calculate standard deviation).</li>
</ol>
</blockquote>
<p>Here is one simple example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">1</span>]: X = np.random.randn(<span class="number">100</span>)</div><div class="line">        print(<span class="string">"Mean:&#123;&#125;\nVariance:&#123;&#125;\n"</span>.format(np.mean(X), np.var(X)))</div><div class="line">Out[<span class="number">1</span>]: Mean:<span class="number">-0.175673121371</span></div><div class="line">        Variance:<span class="number">0.815885946342</span></div></pre></td></tr></table></figure>
<p>where we just get one point… what about using bootstrap? </p>
<p><img src="/images/bootstrap.png" style="width: 600px; height: 300px"> </p>
<p>Ok, now let’s see how bootstrap is incorporated in Bagging.</p>
<blockquote>
<p>We can generate \(B\) bootstrap samples and build a decision tree on each of them, then aggregation to get the final result. Yes we can formulate the idea of bagging into one sentence.</p>
</blockquote>
<p>The prediction of bagging is given by<br>\[<br>\hat{y}_{\text{bag}} = \frac{1}{B}\sum^{B}_{b=1}\hat{y}^{b}<br>\]</p>
<p>for regression problem, and<br>\[<br>\hat{y}_{\text{bag}} = \arg\max_{k}\hat{p}_{k}<br>\]</p>
<p>for classification problem, where \(\hat{p}_{k}\) is the proportion of \(k\) th category (<strong>Majority Vote</strong>).</p>
<p>Then we check if there is any problems in this process. Since we use bootstrap, is that possible that some data is not sampled? Yes it is. Consider we have \(n\) data points, the probability that point \(i\) is not selected in one draw is<br>\[<br>P(i\text{ not selected in one draw}) = 1 - \frac{1}{n}.<br>\]</p>
<p>then the probability that this point is not sampled in bootstrap sample is<br>\[<br>P(i\text{ not sampled in one bootstrap sample}) = \left(1 - \frac{1}{n}\right)^{n}.<br>\]</p>
<p>do you see something familiar? </p>
<p>\[<br>\lim_{n\rightarrow\infty}\left(1-\frac{1}{n}\right)^{n} = \frac{1}{e} \approx 0.367.<br>\]</p>
<p>This implies that approximately 63% of data is used in Bagging (just approximation!). The rest of data that we are not used is called <strong>out-of-bag (OOB)</strong> data. It’s important to know how well does bagging perform on the out-of-bag, we need to estimate <strong>out-of-bag-error</strong>. It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error\(^{[1]}\).</p>
<h4 id="2-2-Random-Forest"><a href="#2-2-Random-Forest" class="headerlink" title="2.2 Random Forest"></a>2.2 Random Forest</h4><p>Then let’s see Random Forests. </p>
<p>The key word is “Random”, which means at <strong>each step</strong> of growing tree (<strong>find a split</strong>), we only use a subset of features rather than all of them, other steps is same as Bagging.</p>
<p>In practice, the sample size for features is denoted as <code>max_features</code> (in <code>sklearn</code> or <code>randomForest (R)</code>), this is one of the hyper-parameters of random forests algorithms. Usually we may started with \(m=\sqrt{p}\).</p>
<h3 id="3-Connection-between-Bagging-and-Random-Forests"><a href="#3-Connection-between-Bagging-and-Random-Forests" class="headerlink" title="3. Connection between Bagging and Random Forests"></a>3. Connection between Bagging and Random Forests</h3><p>If we set the sample size of features equal to the total number of features, random forests is same as bagging.</p>
<p>We can see the difference between bagging and random forests is the number of features. Random forests can <strong>de-correlates</strong> the decision trees using sampling. Since it is possible that there is an important feature and it will dominant all decision trees if we just include all features, the decision trees are less diversity.</p>
<h3 id="4-Summary"><a href="#4-Summary" class="headerlink" title="4. Summary"></a>4. Summary</h3><h4 id="4-1-Pros-and-Cons"><a href="#4-1-Pros-and-Cons" class="headerlink" title="4.1 Pros and Cons"></a>4.1 Pros and Cons</h4><p>The good things about bagging and random forest:</p>
<ul>
<li>Good accuracy because of ensemble mechanism;</li>
<li>Reduce variance because of bootstrap on samples and features;</li>
</ul>
<p>But:</p>
<ul>
<li>Loss of interpretations because of ensemble machanism </li>
</ul>
<h4 id="4-2-Feature-Importance"><a href="#4-2-Feature-Importance" class="headerlink" title="4.2 Feature Importance"></a>4.2 Feature Importance</h4><p>Decision tree training is done by splitting, and splitting is done by selecting the combination of feature and value that reduce the most error (impurity). This quantity could be recorded along with each feature across all decision tree, the higher this quantity is the more important the feature it is. This method is by default used in <code>sklearn</code> and other tree ensemble algorithms such as XGBoost and LightGBM.</p>
<h3 id="5-Implementation"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation</h3><p>The implementations of bagging and random forests are based on decision tree, I invoke <code>DecisionTreeClassifier</code> in <code>sklearn</code> and here is the <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/classification/ensemble-bagging-randomforests/bagging_random-forest.ipynb" target="_blank" rel="external">implementation</a>.</p>
<h3 id="6-References"><a href="#6-References" class="headerlink" title="6. References"></a>6. References</h3><ul>
<li>Gareth James, Daniela Witten, Trevor Hastie, Robert Tishirani, Introduction to Statistical Learning.</li>
<li>Random Forest, <a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank" rel="external">https://en.wikipedia.org/wiki/Random_forest</a></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Classification/">Classification</a><a href="/tags/Ensemble-Algorithms/">Ensemble Algorithms</a><a href="/tags/Decision-Tree/">Decision Tree</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2020 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>