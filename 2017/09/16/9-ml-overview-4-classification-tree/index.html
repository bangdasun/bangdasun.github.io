<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (4) - Classification Tree | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Machine Learning,Decision Tree">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (4) - Classification Tree"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/categories">Categories</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (4) - Classification Tree</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/09/16/9-ml-overview-4-classification-tree/" rel="bookmark">
        <time class="entry-date published" datetime="2017-09-16T16:35:00.000Z">
          2017-09-16
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>There are multiple types of decision tree models for classification problem: CART, ID3, C4.5. The differences include learning algorithm (the way of splitting) and whether it can handle continuous features. In summary, CART can be applied to more general use cases.</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Last time we go through regression tree: using decision tree (CART) to tackle regression problem. Another part of CART is <strong>classification</strong>. To make this blog easier to read, I take binary classification tree as the example, which means we use decision tree to solve binary classification problem.</p>
<h3 id="2-Mathematical-Basis"><a href="#2-Mathematical-Basis" class="headerlink" title="2. Mathematical Basis"></a>2. Mathematical Basis</h3><h4 id="2-1-Comparisons-with-Regression-Tree"><a href="#2-1-Comparisons-with-Regression-Tree" class="headerlink" title="2.1 Comparisons with Regression Tree"></a>2.1 Comparisons with Regression Tree</h4><p>To convert regression problem to classification problem, there are several points we need to clarify:</p>
<ul>
<li>How to predict the class label of a region? In regression, we use the mean of target in the region \(\bar{y}\);</li>
</ul>
<p>For a certain region (which may include both two classes), we define the class label of the region to be the <strong>mode</strong> of labels of data points in the region, a.k.a <strong>“Majority Vote”</strong>. As it is not additive, it cannot be used in GBDT.</p>
<ul>
<li>How to measure the performance of one split, or say which loss function to use? In regression, we use squared loss function;</li>
</ul>
<p>Based on our definition of region label assignment, the error is the proportion of data points in that region that don’t belong to the class that region label indicates, it is <strong>misclassification rate</strong>, the misclassification rate produced by one split is</p>
<p>\[<br>L = \sum^{2}_{r=1}p_{r}\sum_{x_{i}\in R_{r}}\mathbb{I}\left(y_{i}\neq \hat{y}_{r}\right),<br>\]</p>
<p>where \(\hat{y}_{r}\) is the mode of observation labels in region \(r\), \(p_{r}\) is the proportion of observation in the region to the total sample size. This is also know as <strong>0-1 loss</strong>.</p>
<h4 id="2-2-Measures-of-Node-Impurity"><a href="#2-2-Measures-of-Node-Impurity" class="headerlink" title="2.2 Measures of Node Impurity"></a>2.2 Measures of Node Impurity</h4><p>However this measure for error is <strong>not sufficiently sensitive for tree-growing</strong> (mentioned in page 315, Introduction to statistical learning). In practice, two other measures are used:</p>
<ul>
<li><strong>Gini index</strong>:</li>
</ul>
<p>\[<br>L = \sum^{2}_{r=1}p_{r}\sum^{1}_{k=0}p_{rk}(1-p_{rk}).<br>\]</p>
<p>Motivation for Gini index: instead of predicting the most likely class, it predict the <strong>probability for each class</strong>. Gini index is the expected misclassification rate.</p>
<ul>
<li><strong>Information gain</strong></li>
</ul>
<p>\[<br>L = -\sum^{2}_{r=1}p_{r}\sum^{1}_{k=0}p_{rk}\log(p_{rk}).<br>\]</p>
<p>where \(p_{rk}\) is the proportion of class \(k\) in region \(r\). <strong>Note it is just entropy here, not cross entropy</strong>, and the loss reduction is actually <strong>mutual information</strong>. This is the basis of ID3. C4.5 uses a slightly different measure to handle high cardinal categorical features.</p>
<p>We can plot them together (assume there are two regions and they are equal sized).</p>
<p><img src="/images/loss-classification-tree.png" style="width: 650px; height: 400px"> </p>
<p>When the region is mostly one category, Gini index and cross entropy will take a <strong>smaller</strong> value (which means <strong>node impurity is higher</strong>, as shown in the plot), they are better measures of the node purity.</p>
<h4 id="2-3-Loss-Function"><a href="#2-3-Loss-Function" class="headerlink" title="2.3 Loss Function"></a>2.3 Loss Function</h4><p>As mentioned above, Gini index and cross entropy could be used as measure for tree node impurity. And they can also regarded as <strong>loss functions</strong> for classification tree. Specifically, <strong>CART uses Gini index</strong>; <strong>ID3, C4.5 use information gain.</strong> They are supposed to produce different trees, also notice that in <code>sklearn.tree.DecisionTreeClassifier</code>, there is a parameter <code>criterion</code> with options <code>gini</code> and <code>entropy</code>.</p>
<h3 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h3><p>Since we already implemented regression tree before, we just need to update the loss function and prediction function. Here is the <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/classification/classification-decision-tree/CART-C.py" target="_blank" rel="external">source code</a> and <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/classification/classification-decision-tree/CART-C.ipynb" target="_blank" rel="external">demo</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">In [9]: tree = ClassificationTree()</div><div class="line">        tree.build_tree(X, y, 2, 3)</div><div class="line">        tree.print_tree(tree.root)</div><div class="line">Out[9]: [X0 &lt; 6.642287351]</div><div class="line">        -[X0 &lt; 2.771244718]</div><div class="line">        --[[ 0.]]</div><div class="line">        --[X0 &lt; 3.678319846]</div><div class="line">        ---[[ 0.]]</div><div class="line">        ---[[ 0.]]</div><div class="line">        -[X0 &lt; 7.497545867]</div><div class="line">        --[[ 1.]]</div><div class="line">        --[[ 1.]]</div></pre></td></tr></table></figure>
<h3 id="4-References"><a href="#4-References" class="headerlink" title="4. References"></a>4. References</h3><ul>
<li>Gareth James, Daniela Witten, Trevor Hastie, Robert Tishirani, Introduction to Statistical Learning.</li>
<li>Decision Tree Learning, <a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="external">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Decision-Tree/">Decision Tree</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2021 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>