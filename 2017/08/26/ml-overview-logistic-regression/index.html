<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (1) - Logistic Regression | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Classification,Logistic Regression">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (1) - Logistic Regression"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">归档</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (1) - Logistic Regression</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/26/ml-overview-logistic-regression/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-26T21:48:01.000Z">
          2017-08-26
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Simple but important logistic regression!</p>
<p><script src="mathjax-config.js"></script></p>
<p><script src="scripts/MathJax.js"></script></p>


<a id="more"></a>
<p>If the mathematical formula parts just show the TeX code, see this <a href="https://bangdasun.github.io/2017/08/22/about-mathematics-formula-display/" target="_blank" rel="external">post</a>.</p>
<h3 id="Mathematical-Basis"><a href="#Mathematical-Basis" class="headerlink" title="Mathematical Basis"></a>Mathematical Basis</h3><p>We know that supervised learning could be regarded as the process of learning some kinds of mapping \(f()\) between input and output,</p>
<p>\[<br>y = f(X).<br>\]</p>
<p>Logistic regression is a simple (mathematically at least) linear model for classification problems in supervised learning, which means there are some linear relationship exist.</p>
<p>We start from simplest case where there are only two classes, the output \(y\) is binary, if we regard it as a random variable, we will have \(Y\sim \text{Bernoulli}(p)\) with probability density function</p>
<p>\[<br>f(y) = p^{y}(1-p)^{1-y},~\text{where }y\in\{0,1\},~p\in[0,1],<br>\]</p>
<p>then from the probabilistic perspective, we would like to modeling the probability of the output to be positive (actually this is the parameter of the Bernoulli distribution above), i.e.</p>
<p>\[<br>P(y=1|x)=p(x).<br>\]</p>
<p>We could build a linear regression model between \(x\) and \(p(x)\). However it could happens that for some ranges of \(x\), \(p(x)\) is out of \([0,1]\), that’s hard to interpret. One of the ideas is use a function to map \(p(x)\) within \([0,1]\). And one of the choice of the function is called <strong>sigmoid</strong> function \(s(x)\),</p>
<p>\[<br>s(x) = \frac{1}{1+e^{-x}},<br>\]<br>with derivative</p>
<p>\[<br>s’(x) = \frac{e^{-x}}{(1+e^{-x})^{2}}.<br>\]</p>
<p>It’s a “S”-shaped curve where \(s(x)\) is close to 0 for \(x\) goes to \(-\infty\) and close to 1 for \(x\) goes to \(+\infty\).</p>
<p>Therefore logistic regression is actually modeling the probability of outputs to be one of the classes,</p>
<p>\[<br>\begin{aligned}<br>P(y = 1|x) &amp;= \frac{1}{1+\exp(-x\beta)}, \\<br>P(y = 0|x) &amp;= \frac{\exp(-x\beta)}{1+\exp(-x\beta)},<br>\end{aligned}<br>\]</p>
<p>here \(x\) is a row vector of predictors, \(\beta\) is a column vector consists of coefficients of logistic regression. Now assume we have a \(n\times p\) input matrix \(\mathbf{X}\), represents for \(n\) observations and \(p\) features (predictors), where \(\mathbf{x}_{i}\) is the \(i\) th row; an output vector \(\mathbf{y}\) with length \(n\). For observation \(i\), we have<br>\[<br>P(y_{i} = 1|\mathbf{x}_{i}) = \frac{1}{1+\exp(-\mathbf{x}_{i}\beta)},<br>\]<br>then we can do some transformation,<br>\[<br>\begin{aligned}<br>\frac{P(y_{i} = 1|\mathbf{x}_{i})}{P(y_{i} = 0|\mathbf{x}_{i})} = \frac{1}{\exp(-\mathbf{x}_{i}\beta)} = \exp(\mathbf{x}_{i}\beta) \longrightarrow \ln\left[\frac{P(y_{i} = 1|\mathbf{x}_{i})}{P(y_{i} = 0|\mathbf{x}_{i})}\right] = \ln\left[\frac{p(\mathbf{x}_{i})}{1-p(\mathbf{x}_{i})}\right]<br>\end{aligned} = \mathbf{x}_{i}\beta.<br>\]</p>
<p>Do you still remember that linear regression has this form?</p>
<p>\[<br>\mathbf{y} = \mathbf{X}\beta + \epsilon.<br>\]</p>
<p>Yes, this form is similar to linear regression now.</p>
<p>Next we will try to estimate the coefficients \(\beta\). In linear regression with probabilistic view, we use the idea of maximum likelihood estimate (<strong>MLE</strong>) which means parameters (coefficients) should make the likelihood of the observed data to be maximum, i.e.,</p>
<p>\[<br>\hat{\beta} = \arg\max_{\beta} f(\mathbf{X}, \mathbf{y}|\beta)<br>\]</p>
<p>here we can take condition on \(\mathbf{X}\) since that’s the information we know, therefore we get,</p>
<p>\[<br>\hat{\beta} = \arg\max_{\beta} f(\mathbf{X}, \mathbf{y}|\beta) = \arg\max_{\beta} f(\mathbf{y}|\mathbf{X}, \beta)f_{X}(\mathbf{X}) = \arg\max_{\beta} f(\mathbf{y}|\mathbf{X}, \beta).<br>\]</p>
<p>Hence, for logistic regression, the coefficient should be<br>\[<br>\hat{\beta} = \arg\max_{\beta}f(\mathbf{y}) = \arg\max_{\beta}\prod^{n}_{i=1}f(y_{i})<br>\]<br>where we assume the observations are independent.</p>
<p>The likelihood function we are look for is<br>\[<br>L(\beta) = \prod^{n}_{i=1}f(y_{i}) = \prod^{n}_{i=1}[1-p(\mathbf{x}_{i})]^{1-y_{i}}p(\mathbf{x}_{i})^{y_{i}},<br>\]<br>take a logarithm, we have<br>\[<br>\begin{aligned}<br>\ln L(\beta) =~&amp;\sum^{n}_{i=1}(1-y_{i})\ln[1-p(\mathbf{x}_{i})] + \sum^{n}_{i=1}y_{i}\ln p(\mathbf{x_{i}}) \\<br>             =~&amp;\sum^{n}_{i=1}\ln[1-p(\mathbf{x}_{i})] + \sum^{n}_{i=1}y_{i}\ln\left[\frac{p(\mathbf{x}_{i})}{1-p(\mathbf{x}_{i})}\right] \\<br>             =~&amp;\sum^{n}_{i=1}\ln\left[\frac{\exp(-\mathbf{x}_{i}\beta)}{1+\exp(-\mathbf{x}_{i}\beta)}\right] + \mathbf{y}^\top\mathbf{X}\beta<br>\end{aligned}<br>\]</p>
<p>and actually the optimization problem is equivalent to </p>
<p>\[<br>\hat{\beta} = \arg\min_{\beta} \left(-\sum^{n}_{i=1}\ln\left[\frac{\exp(-\mathbf{x}_{i}\beta)}{1+\exp(-\mathbf{x}_{i}\beta)}\right] - \mathbf{y}^\top\mathbf{X}\beta\right)<br>\]</p>
<p>we can regard this as minimize the <strong>loss function</strong>. </p>
<p>Now the machine learning problem comes to the calculus problem, to find the minima of the function, we take the derivative/gradient and set it to 0…however unfortunately there is no explicit solution to this function, therefore numerical algorithms like gradient descent (<strong>GD</strong>) works.</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Here are two scripts I wrote that can solve this problem, one is for <a href="https://github.com/bangdasun/Statistical-machine-learning/blob/master/algorithms/optimization/gradient-descent/self-defined.R" target="_blank" rel="external">gradient descent algorithm</a> and the other is <a href="https://github.com/bangdasun/Statistical-machine-learning/blob/master/algorithms/classification/logistics-regression/self-defined-binary.R" target="_blank" rel="external">fit logistic regression</a>.</p>
<p>Also <code>R</code> and <code>sklearn.linear_model (Python)</code> provide the functions. In R we have <code>glm()</code> with <code>family = binomial</code> and in <code>sklearn</code> we just need to import <code>LogisticRegression</code>. </p>
<h3 id="More-Discussions"><a href="#More-Discussions" class="headerlink" title="More Discussions"></a>More Discussions</h3><p>I will discuss the pros and cons of logistic regression as well as some practical issues, more theoretical backgrounds next time.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Classification/">Classification</a><a href="/tags/Logistic-Regression/">Logistic Regression</a>
    </span>
    

    </div>

    
  </div>
</article>


    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2017 Bangda
    
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>

</footer>
    
  </div>
</div>
</body>
</html>