<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (1) - Logistic Regression | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Classification,Linear Models,Logistic Regression">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (1) - Logistic Regression"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (1) - Logistic Regression</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/26/6-ml-overview-1-logistic-regression/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-26T22:48:01.000Z">
          2017-08-26
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Simple but important logistic regression</p>
<a id="more"></a>
<h3 id="1-Mathematical-Basis"><a href="#1-Mathematical-Basis" class="headerlink" title="1. Mathematical Basis"></a>1. Mathematical Basis</h3><p>We know that supervised learning could be regarded as the process of learning some kinds of mapping \(f\) between input and output,</p>
<p>\[<br>y = f(X).<br>\]</p>
<p>And logistic regression is a simple linear model for classification problems, which means \(f\) is learn.</p>
<h4 id="1-1-Probabilistic-Motivation"><a href="#1-1-Probabilistic-Motivation" class="headerlink" title="1.1 Probabilistic Motivation"></a>1.1 Probabilistic Motivation</h4><p>We start from simplest case where there are only two classes, the output \(y\) is binary, if we consider it as a random variable, we will have \(Y\sim \text{Bernoulli}(p)\) with probability mass function</p>
<p>\[<br>f(y) = p^{y}(1-p)^{1-y},~\text{where }y\in\{0,1\},~p\in[0,1],<br>\]</p>
<p>then from the probabilistic perspective, we would like to modeling the probability of the output to be positive (actually this is the parameter of the Bernoulli distribution above), i.e.</p>
<p>\[<br>P(y=1|x)=p(x).<br>\]</p>
<p>We could build a linear regression model between \(x\) and \(p(x)\). However it could happens that for some ranges of \(x\), \(p(x)\) is out of \([0,1]\), that’s hard to interpret. One of the ideas is use a function to map \(p(x)\) within \([0,1]\). And one of the choice of the function is called <strong>sigmoid</strong> function \(s(x)\),</p>
<p>\[<br>s(x) = \frac{1}{1+e^{-x}},<br>\]<br>with derivative</p>
<p>\[<br>s’(x) = \frac{e^{-x}}{(1+e^{-x})^{2}}.<br>\]</p>
<p>It’s a “S”-shaped curve where \(s(x)\) is close to 0 for \(x\) goes to \(-\infty\) and close to 1 for \(x\) goes to \(+\infty\).</p>
<p>Therefore logistic regression is actually modeling the probability of outputs to be one of the classes,</p>
<p>\[<br>\begin{aligned}<br>P(y = 1|x) &amp;= \frac{1}{1+\exp(-x\beta)}, \\<br>P(y = 0|x) &amp;= \frac{\exp(-x\beta)}{1+\exp(-x\beta)},<br>\end{aligned}<br>\]</p>
<p>here \(x\) is a row vector of predictors, \(\beta\) is a column vector consists of coefficients of logistic regression. Now assume we have a \(n\times p\) input matrix \(\mathbf{X}\), represents for \(n\) observations and \(p\) features (predictors), where \(\mathbf{x}_{i}\) is the \(i\) th row; an output vector \(\mathbf{y}\) with length \(n\). For observation \(i\), we have<br>\[<br>P(y_{i} = 1|\mathbf{x}_{i}) = \frac{1}{1+\exp(-\mathbf{x}_{i}\beta)},<br>\]<br>then we can do some transformation,<br>\[<br>\begin{aligned}<br>\frac{P(y_{i} = 1|\mathbf{x}_{i})}{P(y_{i} = 0|\mathbf{x}_{i})} = \frac{1}{\exp(-\mathbf{x}_{i}\beta)} = \exp(\mathbf{x}_{i}\beta) \longrightarrow \ln\left[\frac{P(y_{i} = 1|\mathbf{x}_{i})}{P(y_{i} = 0|\mathbf{x}_{i})}\right] = \ln\left[\frac{p(\mathbf{x}_{i})}{1-p(\mathbf{x}_{i})}\right]<br>\end{aligned} = \mathbf{x}_{i}\beta.<br>\]</p>
<p>Do you still remember that linear regression has this form?</p>
<p>\[<br>\mathbf{y} = \mathbf{X}\beta + \epsilon.<br>\]</p>
<p>Yes, this form is similar to linear regression now, the difference is the target is the <strong>odds ratio</strong> rather that raw target.</p>
<h4 id="1-2-Parameters-Estimation"><a href="#1-2-Parameters-Estimation" class="headerlink" title="1.2 Parameters Estimation"></a>1.2 Parameters Estimation</h4><p>Next we will try to estimate the coefficients \(\beta\). In linear regression with probabilistic view, we use the idea of maximum likelihood estimate (<strong>MLE</strong>) which means parameters (coefficients) should make the likelihood of the observed data to be maximum, i.e.,</p>
<p>\[<br>\hat{\beta} = \arg\max_{\beta} f(\mathbf{X}, \mathbf{y}|\beta)<br>\]</p>
<p>here we can take condition on \(\mathbf{X}\) since that’s the information we know, therefore we get,</p>
<p>\[<br>\hat{\beta} = \arg\max_{\beta} f(\mathbf{X}, \mathbf{y}|\beta) = \arg\max_{\beta} f(\mathbf{y}|\mathbf{X}, \beta)f_{X}(\mathbf{X}) = \arg\max_{\beta} f(\mathbf{y}|\mathbf{X}, \beta).<br>\]</p>
<p>Hence, for logistic regression, the coefficient should be<br>\[<br>\hat{\beta} = \arg\max_{\beta}f(\mathbf{y}) = \arg\max_{\beta}\prod^{n}_{i=1}f(y_{i})<br>\]<br>where we assume the observations are independent.</p>
<p>The likelihood function we are look for is<br>\[<br>L(\beta) = \prod^{n}_{i=1}f(y_{i}) = \prod^{n}_{i=1}[1-p(\mathbf{x}_{i})]^{1-y_{i}}p(\mathbf{x}_{i})^{y_{i}},<br>\]<br>take a logarithm, we have<br>\[<br>\begin{aligned}<br>\ln L(\beta) =~&amp;\sum^{n}_{i=1}(1-y_{i})\ln[1-p(\mathbf{x}_{i})] + \sum^{n}_{i=1}y_{i}\ln p(\mathbf{x_{i}}) \\<br>​             =~&amp;\sum^{n}_{i=1}\ln[1-p(\mathbf{x}_{i})] + \sum^{n}_{i=1}y_{i}\ln\left[\frac{p(\mathbf{x}_{i})}{1-p(\mathbf{x}_{i})}\right] \\<br>​             =~&amp;\sum^{n}_{i=1}\ln\left[\frac{\exp(-\mathbf{x}_{i}\beta)}{1+\exp(-\mathbf{x}_{i}\beta)}\right] + \mathbf{y}^\top\mathbf{X}\beta<br>\end{aligned}<br>\]</p>
<p>and actually the optimization problem is equivalent to </p>
<p>\[<br>\hat{\beta} = \arg\min_{\beta} \left(-\sum^{n}_{i=1}\ln\left[\frac{\exp(-\mathbf{x}_{i}\beta)}{1+\exp(-\mathbf{x}_{i}\beta)}\right] - \mathbf{y}^\top\mathbf{X}\beta\right)<br>\]</p>
<h4 id="1-3-Learning-with-Loss-Function"><a href="#1-3-Learning-with-Loss-Function" class="headerlink" title="1.3 Learning with Loss Function"></a>1.3 Learning with Loss Function</h4><p>We can regard this as minimize the <strong>loss function</strong>. Looks a little bit complex, we can just look at the first step of log likelihood,</p>
<p>\[<br>\ln L(\beta) = \sum^{n}_{i=1}(1-y_{i})\ln[1-p(\mathbf{x}_{i})] + \sum^{n}_{i=1}y_{i}\ln p(\mathbf{x_{i}}),<br>\]</p>
<p>this is very close to the form of <strong>cross entropy</strong>! The gradient of log likelihood is easier to derive from this form,</p>
<p>\[<br>\nabla_{\beta}\ln L(\beta) = \sum^{n}_{i=1}\left(y_{i} - p(\mathbf{x}_{i})\right)\mathbf{x}_{i},<br>\]</p>
<p>note that</p>
<p>\[<br>\frac{\partial p(\mathbf{x}_{i})}{\partial \beta} = \mathbf{x}_{i}p(\mathbf{x}_{i})\left(1 - p(\mathbf{x}_{i})\right).<br>\]</p>
<p>Now the machine learning problem comes to the calculus problem, to find the minima of the function, we take the derivative/gradient and set it to 0…however unfortunately there is no explicit solution to this function, therefore numerical algorithms such as gradient descent are the way to go.</p>
<h3 id="2-Implementation"><a href="#2-Implementation" class="headerlink" title="2. Implementation"></a>2. Implementation</h3><p>Here are two scripts I wrote that can solve this problem, one is for <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/optimization/gradient-descent/gradient-descent.R" target="_blank" rel="external">gradient descent algorithm</a> and the other is <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/classification/logistics-regression/logistic-regression.R" target="_blank" rel="external">fit logistic regression</a>.</p>
<p>Also <code>R</code> and <code>sklearn.linear_model (Python)</code> provide the functions. In R we have <code>glm()</code> with <code>family = binomial</code> and in <code>sklearn</code> we just need to import <code>LogisticRegression</code>. </p>
<p>Here is one example of applying logistic regression on a binary classification problem. The two classes (blue and red) are linear separable. I draw the decision boundary generated by both built-in function in <code>R</code> (<code>glm()</code>) and the function I implemented by myself: solid line is the the built-in function and dashed line is the function I implemented. </p>
<p><img src="/images/logistic-regression-decision-boundary.png" style="width: 800px; height: 480px"> </p>
<p>And you can see that logistic regression is a linear classification method: the points on decision boundary is when \(P(y_{i}=1|\mathbf{x}_{i})=1/2\), where leads to \(\mathbf{x}_{i}\beta=0\), therefore it’s produce linear decision boundary.</p>
<h3 id="3-Discussions"><a href="#3-Discussions" class="headerlink" title="3. Discussions"></a>3. Discussions</h3><p>I will discuss the pros and cons of logistic regression as well as some practical issues, more theoretical backgrounds next time.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Classification/">Classification</a><a href="/tags/Linear-Models/">Linear Models</a><a href="/tags/Logistic-Regression/">Logistic Regression</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>