<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (6) - Support Vector Machines | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Classification,SVM">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (6) - Support Vector Machines"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">归档</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (6) - Support Vector Machines</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/10/21/ml-overview-support-vector-machines/" rel="bookmark">
        <time class="entry-date published" datetime="2017-10-21T17:32:13.000Z">
          2017-10-21
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>A classification method with straightforward geometric intuition.</p>
<a id="more"></a>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>If you’ve read or taken the famous machine learning open course CS229 taught by Andrew Ng, and I ask you what’s first algorithm that looks more complicated than the previous one, I bet you will say Support Vector Machines (SVM) since there are lots of mathematics / algebra in steps. In this post, I will try to make SVM more intuitive and understandable.</p>
<p>Similar with the Logistic Regression we discussed before, SVM is also an algorithm that try to fit a decision boundary to separate the data: define the parameter and loss function, and use optimization algorithm to minimize the loss, the result would be a decision boundary specified by the parameters.</p>
<p>The extra point is classify different classes as far as possible, i.e. we need to maximize the distance between the decision boundaries to each class. Take binary classification as an example, our ideal decision boundary should locate at the middle of two classes, like the following figure</p>
<p><img src="/images/decision-boundary.png" style="width: 850px; height: 450px"> </p>
<p>As the figure shows, we will first find a closest pair of points between two classes, and place the decision boundary perpendicular to the connection line between these two points. In order to maximize the distance of decision boundary to each class, we should place the decision boundary at the middle point of the connection line. In this example, the maximum distance is \(d\), and we define it as <strong>margin</strong>. This classifier is known as <strong>Maximum Margin Classifier</strong>, it’s a special case of SVM. </p>
<p>Looks nice, but what if the data is not linearly separable? In that case we are not able to conduct the above procedure. Don’t worry, we will back to this point later.</p>
<h3 id="Mathematical-Basis"><a href="#Mathematical-Basis" class="headerlink" title="Mathematical Basis"></a>Mathematical Basis</h3><p>First we need some geometric backgrounds. Usually we can specify the decision boundary using a norm vector \(\mathbf{v}\), and consider a data point \(\mathbf{x}\)</p>
<p><img src="/images/decision-boundary2.png" style="width: 850px; height: 450px"> </p>
<p>from the figure we can get the following relation:</p>
<p>\[<br>\cos{\theta} = \frac{\mathbf{v}^{\top}\mathbf{x}}{||\mathbf{v}||_{2}||\mathbf{x}||_{2}}<br>\]</p>
<p>where \(||\mathbf{v}||_{2}\) means the length of vector \(\mathbf{v}\). So what’s the distance of data point \(\mathbf{x}\) to the decision boundary? We have</p>
<p>\[<br>d = ||\mathbf{x}||_{2} \cos{\theta} = \frac{\mathbf{v}^{\top}\mathbf{x}}{||\mathbf{v}||_{2}}<br>\]</p>
<p>Also we can observed that whether \(\theta\) is greater than \(\pi/2\) will indicate which side of the decision boundary that \(\mathbf{x}\) located in. This motivate us that the classifier could be</p>
<p>\[<br>f(\mathbf{x}) = \mathbb{I}\{\mathbf{v}^{\top}\mathbf{x} &gt; 0\}.<br>\]</p>
<p>Next we can enter the core part. To fit a decision boundary of SVM (binary), we need to solve the following optimization problem:</p>
<p>\[<br>\begin{aligned}<br>&amp;\min_{\mathbf{v}, c}~||\mathbf{v}||_{2},\\<br>\text{ s.t.  }&amp; y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i} -c)\geq 1, ~i = 1,2,\cdots,n<br>\end{aligned}<br>\]</p>
<p>where \(y_{i}\) is class label that takes in \(\{-1, +1\}\) (if we use 0 as before it will get 0 and we will not know which class that this point belongs to). Also we fixed \(\mathbf{v}^{\top}\mathbf{x}=1\), therefore we just need to minimize the denominator.</p>
<p>And we define the correct classification to be  \(y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i} -c)\) to be greater than 1 rather than 0. This is useful in the later discussion, here you just need to know that we generate two parallel lines at both sides of the decision boundary. Therefore we will get the classification result like this,</p>
<p><img src="/images/decision-boundary3.png" style="width: 850px; height: 450px"> </p>
<p>The classifier we got is known as <strong>Maximum Margin Classifier</strong>. The length of the margin is the distance between the solid line (decision boundary) and the dashed line. The constraints here means the points cannot exceed the margin. And the points on the margin are called <strong>support vectors</strong>.</p>
<p>Solving this optimization problem is not easy since the constraints are functions. Using <strong>Lagrange multipliers</strong> we can rewrite optimization problem like this</p>
<p>\[<br>\min_{\mathbf{v}, c} ~\ell = ||\mathbf{v}||_{2} - \sum^{n}_{i=1}\alpha_{i}[y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i} -c) - 1]<br>\]</p>
<p>take the derivatives respect to \(\mathbf{v}\) and \(c\) we can get</p>
<p>\[<br>\begin{aligned}<br>\frac{d\ell}{d\mathbf{v}} = \mathbf{v} - \sum^{n}_{i=1}\alpha_{i}y_{i}\mathbf{x}_{i}=0&amp;~\rightarrow \mathbf{v}=\sum^{n}_{i=1}\alpha_{i}y_{i}\mathbf{x}_{i} \\<br>\frac{d\ell}{d c} = -\sum^{n}_{i=1}\alpha_{i}y_{i}=0&amp;~\rightarrow \sum^{n}_{i=1}\alpha_{i}y_{i}=0<br>\end{aligned}<br>\]</p>
<p>Plugging into the previous optimization problem we can again rewrite our optimization problem like this </p>
<p>\[<br>\begin{aligned}<br>&amp;\max_{\alpha_{i}}~L = \sum^{n}_{i=1}\alpha_{i} - \sum^{n}_{i=1}\sum^{n}_{j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}\mathbf{x}_{i}^{\top}\mathbf{x}_{j},\\<br>\text{ s.t.  }&amp;~\sum^{n}_{i=1}\alpha_{i}y_{i}=0,~i=1,2,\cdots,n.<br>\end{aligned}<br>\]</p>
<p>The solution of this optimization problem exists only the data is linearly separable. If the data is not perfectly linearly separable, we need to modify the constraints and objective functions of the optimization problem.</p>
<p>\[<br>\begin{aligned}<br>&amp;\min_{\mathbf{v}, c, \epsilon_{i}}~||\mathbf{v}||_{2} + \alpha\sum^{n}_{i=1}\epsilon_{i},\\<br>\text{ s.t.  }&amp; y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i} -c)\geq 1-\epsilon_{i}, ~i = 1,2,\cdots,n<br>\end{aligned}<br>\]</p>
<p>where \(\alpha\) is our tolerance to the incorrect classification cases; \(\epsilon_{i}\) is called slack variable. \(\epsilon_{i}=0\) when the point is located on the margin. This allows points exist between the decision boundary and margin, even on the other side of the decision boundary.</p>
<p>No we can see if the point is correct classified, we have</p>
<p>\[<br>y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i}-c)\geq 1\rightarrow\epsilon_i=0,<br>\]</p>
<p>and if the point is not correct classified, we have</p>
<p>\[<br>y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i}-c) &lt; 1\rightarrow\epsilon_{i} = 1-y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i}-c)<br>\]</p>
<p>therefore we have</p>
<p>\[<br>\epsilon_{i} = \max\{0, 1 - y_{i}(\mathbf{v}^{\top}\mathbf{x}_{i} - c)\}<br>\]</p>
<p>We can solve the problem by minimize the overall \(\epsilon_{i}\) and actually we get the loss function of SVM, </p>
<p>\[<br>L(y_{i}, \hat{y}_{i}) = \max\{0, 1 - y_{i}\hat{y}_{i}\},<br>\]</p>
<p>this is called <strong>hinge loss</strong> and it’s a upper bound of <strong>0-1 loss</strong>. The following part is easy to go.</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Here is one implementation of <a href="https://github.com/bangdasun/Algorithms/tree/master/machine-learning/classification/svm" target="_blank" rel="external">linear SVM</a> and <a href="https://github.com/bangdasun/Algorithms/blob/master/machine-learning/classification/svm/linear-svm.ipynb" target="_blank" rel="external">demo</a>.</p>
<h3 id="More-Discussions"><a href="#More-Discussions" class="headerlink" title="More Discussions"></a>More Discussions</h3><ul>
<li>Kernel </li>
<li>Convex Optimization</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Classification/">Classification</a><a href="/tags/SVM/">SVM</a>
    </span>
    

    </div>

    
  </div>
</article>


    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2018 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>

</footer>
    
  </div>
</div>
</body>
</html>