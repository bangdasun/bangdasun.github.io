<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Common Metrics in Binary Classification Problems | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Machine Learning,Model Evaluation,Metrics">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Common Metrics in Binary Classification Problems"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Common Metrics in Binary Classification Problems</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/07/24/common-metrics-supervised-learning/" rel="bookmark">
        <time class="entry-date published" datetime="2017-07-25T03:48:17.000Z">
          2017-07-24
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Are you familiar with Recall/Precision/F1-score?</p>
<a id="more"></a>
<p>In classification problem, we usually start analysis the performance of the model / algorithm from the <strong>confusion matrix</strong>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; confusion_matrix</div><div class="line">         pred 0 pred 1</div><div class="line">actual 0     55      4</div><div class="line">actual 1      3     43</div></pre></td></tr></table></figure>
<p>here I denote “0” as <strong>negative</strong>, “1” as <strong>positive</strong>, they are the classes in our classification problem. The confusion matrix here says:</p>
<blockquote>
<p>55 observations are correctly classified as “0”, they are <strong>true negative</strong> ;<br>43 observations are correctly classified as “1”, they are <strong>true positive</strong>;<br>4 observations are incorrectly classified as “1”, they are <strong>false positive</strong>;<br>3 observations are incorrectly classified as “0”, they are <strong>false negative</strong>. </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; confusion_matrix</div><div class="line">         pred 0  pred 1     </div><div class="line">actual 0 &quot;TN&quot;    &quot;FP&quot;</div><div class="line">actual 1 &quot;FN&quot;    &quot;TP&quot;</div></pre></td></tr></table></figure>
<p>We always define the class that we are interested in as “Positive”. For instance, when we want to detect the spam, the spam would be “Positive”; when we want to predict whether the user will click the link, action of click would be “Positive”. </p>
<p>The first measure we will always use is just the <strong>Accuracy</strong>, which is the proportion of correctly classified observations, here we have</p>
<p>\[<br>\text{Accuracy} = \frac{55+43}{55+43+4+3} = 93.33\%,<br>\]</p>
<p>very easy and straight-forward.</p>
<p>Next we can use <strong>true positive rate (TPR)</strong> and <strong>false positive rate (FPR)</strong>, they are defined as:</p>
<p>\[<br>\text{TPR} = \frac{\text{true positive}}{\text{positive}} = \frac{\text{true positive}}{\text{true positive} + \text{false negative}} = \frac{43}{43+3} = 93.48\%,<br>\]</p>
<p>\[<br>\text{FPR} = \frac{\text{false positive}}{\text{negative}} = \frac{\text{false positive}}{\text{false positive} + \text{true negative}} = \frac{4}{4+55} = 6.78\%,<br>\]</p>
<p>where TPR can be viewed as gauge of “Type I error” (reject true) and FPR corresponds to “Type II error” (failure to reject false). TPR has another name called <strong>Recall</strong>, also known as <strong>Sensitivity</strong>, and 1 - FPR is known as <strong>Specificity</strong>,</p>
<p>From the perspective of confusion matrix, we can see the denominator of FPR is the summation of <strong>first row</strong> of confusion matrix; the denominator of TPR is the summation of <strong>second row</strong> of confusion matrix.</p>
<p>Based on TPR and FPR, we can draw a plot named <strong>ROC</strong> (receiver operating characteristic), where the x-axis is FPR and y-axis is TPR. ROC is plotted at various threshold settings, the ideal point is at (0, 1), means for nice performance, FPR should be as small as possible and TPR should be as large as possible. Also we have <strong>AUC</strong> (area under curve) which is the area under ROC curve. ROC and AUC are widely used in industries.</p>
<p>Next, we can define <strong>Precision</strong>, which is the proportion of all true positive. Sometimes two classes are not “equally weighted” in our problem, which means we care more about one type of class. </p>
<p>\[<br>\text{Precision} = \frac{\text{true positive}}{\text{true positive} + \text{false positive}} = \frac{43}{43+4} = 91.49\%,<br>\]</p>
<p>where the denominator is the <strong>second column</strong> of confusion matrix. Typically we can also draw Precision versus Recall which is known as <strong>PRC</strong>.</p>
<p>Finally we have one more important metric called <strong>F1-score</strong>, which combine Recall and Precision together, it’s the harmonic mean of Recall and Precision:</p>
<p>\[<br>\text{F1-score} = \frac{2\times\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}=\frac{2\times43/47\times43/46}{43/47+43/46} = 0.9247.<br>\]</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><p><a href="https://chrisalbon.com/machine-learning/precision_recall_and_F1_scores.html" target="_blank" rel="external">https://chrisalbon.com/machine-learning/precision_recall_and_F1_scores.html</a></p>
</li>
<li><p><a href="http://shahramabyari.com/2016/02/22/measuring-performance-of-classifiers/#comments" target="_blank" rel="external">http://shahramabyari.com/2016/02/22/measuring-performance-of-classifiers/#comments</a></p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank" rel="external">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></p>
</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Model-Evaluation/">Model Evaluation</a><a href="/tags/Metrics/">Metrics</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>