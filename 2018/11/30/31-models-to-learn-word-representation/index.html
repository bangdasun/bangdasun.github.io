<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Models to Learn Word Representation | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="NLP,Deep Learning,Word Embeddings">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Models to Learn Word Representation"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Models to Learn Word Representation</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/11/30/31-models-to-learn-word-representation/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-01T00:34:41.000Z">
          2018-11-30
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>[Paper reading] Efficient Estimation of Word Representations in Vector Space</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p><strong>Word2Vec</strong> was first published in 2013, but the idea of word embeddings was presented much earlier. Back to 2003, Yoshua Bengio et al presented a Neural Network Language Model and word feature vectors were learned during the training (my previous note is the reading summary of this paper). That time the word feature vectors are just “By-product”, and the model didn’t attract too much attention - interesting to see how things changed over decades!</p>
<p>There are several papers related to Word2Vec, here I pick two of them:</p>
<ul>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="external">Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781</a></li>
<li><a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="external">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality arXiv:1310.4546</a></li>
</ul>
<p>This time I’ll summarize the former one.</p>
<h3 id="2-Highlights"><a href="#2-Highlights" class="headerlink" title="2. Highlights"></a>2. Highlights</h3><p>Two new models are presented to learn word representations: <strong>Continuous Bag-of-Words (CBOW)</strong> and <strong>Skip-gram</strong>.</p>
<h4 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h4><p>Compared with other methods: Neural Nets perform better than LSA for preserving linear regularities; LDA is computationly expensive on large dataset.</p>
<h4 id="2-2-Model-Architecture"><a href="#2-2-Model-Architecture" class="headerlink" title="2.2 Model Architecture"></a>2.2 Model Architecture</h4><p>Figure from paper:</p>
<center><br>  <img src="/images/cbow-skipgram.PNG" style="width: 700px; height: 400px"> <br></center>

<h5 id="2-2-1-Continuous-Bag-of-Words-CBOW"><a href="#2-2-1-Continuous-Bag-of-Words-CBOW" class="headerlink" title="2.2.1 Continuous Bag-of-Words (CBOW)"></a>2.2.1 Continuous Bag-of-Words (CBOW)</h5><p>CBOW will <strong>predict the current word</strong> \(w_{t}\) <strong>based on its context</strong> (history words and future words). The projection is simply a <strong>sum</strong> operation - add all word vectors of context. The order of words in the history won’t influence the projection.</p>
<p>As you can see, CBOW architecture is similar with the NNLM (Neural Nets Language Model) presented by Bengio et al. The difference is: non-linear hidden layer is removed (replaced with simply linear activation), projection layer shared for all words.</p>
<h5 id="2-2-2-Skip-gram"><a href="#2-2-2-Skip-gram" class="headerlink" title="2.2.2 Skip-gram"></a>2.2.2 Skip-gram</h5><p>Skip-gram will <strong>predict the context based on current word</strong> \(w_{t}\), therefore no sum operation in the projection. The range of the context need to be tuned. Set \(C\) as the maximum distance of the words, thus if \(C=5\), for eaching training word we will select randomly a number \(R\) between 1 and \(C\), then use \(R\) words from history and \(R\) words from the future of the current word as labels.</p>
<h4 id="2-3-Conclusions"><a href="#2-3-Conclusions" class="headerlink" title="2.3 Conclusions"></a>2.3 Conclusions</h4><ul>
<li>When train high-dimension word vectors on a large amount of data, the resulting vectors can be used to answer <strong>subtle semantic relationships between words</strong> (such as city - country pairs)</li>
<li>To measure the quality of word vectors, define 5 types of semantic questions (e.g. king - queen, brother - sister) and 9 types of syntactic questions (e.g. apparent - apparently, great - greater). Evaluate the overall accuracy</li>
<li>To estimate the best choice of model architecture, evaluate models trained on subset of training data with vocabulary size restrict to most frequent 30000 words</li>
<li>Increase word vector dimension and amount of training data together will get better accuracy</li>
<li>CBOW works better than NNLM on <strong>syntactic</strong> tasks, about the same on <strong>semantic</strong> one; Skip-gram works slightly worse on <strong>syntactic</strong> task than CBOW (but still better than NNLM), and much better on <strong>semantic</strong> part of the test than all the other models</li>
<li>Advantage of models: train high quality word vectors using very simple structure Neural Nets, reduce the computation complexity</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>, <a href="/categories/Machine-Learning/Deep-Learning/">Deep Learning</a>, <a href="/categories/Machine-Learning/Deep-Learning/NLP/">NLP</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/NLP/">NLP</a><a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Word-Embeddings/">Word Embeddings</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2020 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>