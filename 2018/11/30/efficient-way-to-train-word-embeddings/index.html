<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Efficient Way to Train Word Embeddings | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Deep Learning,NLP,Word Embeddings">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Efficient Way to Train Word Embeddings"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Efficient Way to Train Word Embeddings</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/11/30/efficient-way-to-train-word-embeddings/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-01T03:55:44.000Z">
          2018-11-30
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>[Paper reading] Distributed Representations of Words and Phrases and their Compositionality</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>This is the second paper I read about <strong>Word2Vec</strong>. In the last paper reading note, Mikolov et al presented two models to learn word distributed representations, a.k.a. word embeddings. However, there are several issues - hard to train the model efficiently, cannot get high quality word vectors, etc. In this paper, several tricks are introduced to improve the model training efficiency.</p>
<h3 id="2-Highlights"><a href="#2-Highlights" class="headerlink" title="2. Highlights"></a>2. Highlights</h3><p>Some tricks and results:</p>
<ul>
<li>by subsampling of frequent words, obtain significant speedup, also learn more regular word representation</li>
<li>a more efficient computation compared with full softmax: hierarchical softmax; simple alternative to hierarchical softmax: negative sampling</li>
<li>for inherent limitation of word representation (indifference to word order and their inability to represent idiomatic phrases, present a simple method for finding phrases in text, show that learning good vector representations for millions of phrases is possible)</li>
<li>simple vector addition can produce meaningful results (e.g. capital of German gives “Berlin”)</li>
</ul>
<h4 id="2-1-Skip-gram-Model"><a href="#2-1-Skip-gram-Model" class="headerlink" title="2.1 Skip-gram Model"></a>2.1 Skip-gram Model</h4><center><br>  <img src="/images/skipgram.PNG" style="width: 700px; height: 300px"> <br></center>

<p>As shown last time, skip-gram model is predict the context based on current word \(w_{t}\). In formal, find word representations that are useful for predicting surrounding words in a sentence or document. Given a sequence of training words \(w_{1}, w_{2},\cdots, w_{T}\), maximize</p>
<p>\[<br>\frac{1}{T} \sum^{T}_{t=1}\sum_{-c\leq j\leq, j\neq 0}\log P\left(w_{t+j}|w_{t}\right),<br>\]</p>
<p>\(c\) is the size of training context, larger \(c\) results in more training examples and thus can lead to higher accuracy.</p>
<p>The probability of context given current word is defined as </p>
<p>\[<br>P(w_{O}|w_{I}) = \frac{\exp\left(v’^\top_{w_{O}}v_{w_{I}}\right)}{\sum^{W}_{w=1}\exp\left(v’^\top_{w_{w}}v_{w_{I}}\right)}<br>\]</p>
<p>\(v_{w}\) and \(v’_{w}\) are the “input” and “output” vector representation of \(w\), you can simple think it to be the left side weight and right side weight around the projection layer. \(W\) is the number of words in vocabulary.</p>
<h4 id="2-2-Hierarchical-Softmax"><a href="#2-2-Hierarchical-Softmax" class="headerlink" title="2.2 Hierarchical Softmax"></a>2.2 Hierarchical Softmax</h4><p>This is a computationally efficient approximation for full softmax. In full softmax, the width of the layer is \(W\), and the index of highest probability is the prediction index of word in vocabulary. Use better data structure could improve the computation. Bengio et al previous use binary tree representation. Here they use similar way, use Huffman tree to represent the output layer with \(W\) words as its leaves, for each node, explicitly represents the relative probabilities of its child nodes. So calculate the softmax output of a word is actually along a path from the root to leaf.</p>
<p>Let \(n(w, j)\) be the \(j\)th node on path from root to \(w\), \(L(w)\) be the length of the path (equivalent to the depth of the tree which is \(O(\log W)\)), so \(n(w, 1)\) is root, \(n(w, L(w))\) is \(w\). For any inner node, let \(ch(n)\) be an arbitrary fixed child of \(n\) and let \(\mathbb{[} x\mathbb{]}\) be 1 if \(x\) is true and -1 otherwise. Then hierarchical softmax define</p>
<p>\[<br>P(w_{O}|w_{I}) = \prod^{L(w)-1}_{j=1}\sigma\left(\mathbb{[}n(w, j+1) = ch\left(n(w, j)\right) \mathbb{]}v’^\top_{n(w,j)} v_{w_{I}}\right)<br>\]</p>
<p>where \(sigma(x) = 1 / (1 + e^{-x})\) is the sigmoid function. It can be verified that \(\sum^{W}_{w=1}P(w|w_{I}) = 1\).</p>
<p>The probability definition looks complex, but the intuition is simple. Use the figure from<br><a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="external">Xin Rong, word2vec Parameter Learning Explained</a> (this is the paper that strongly recommended if you have difficulities in understanding Word2Vec),</p>
<center><br>  <img src="/images/hierarchicalsoftmax.PNG" style="width: 700px; height: 200px"> <br></center>

<p>it’s very straightforward to see that the probability of output to be \(w_{2}\) is</p>
<p>\[<br>P(w_{O}=w_{2}) = P\left(n(w_{2}, 1)\text{ goes left}\right)P\left(n(w_{2}, 2)\text{ goes left}\right)P\left(n(w_{2}, 3)\text{ goes right}\right)<br>\]</p>
<p>this is equivalent to</p>
<p>\[<br>P(w_{O}=w_{2}) = \sigma\left(v’^\top_{n(w_{2}, 1)}v_{w_{I}}\right) \sigma\left(v’^\top_{n(w_{2}, 2)}v_{w_{I}}\right) \sigma\left(-v’^\top_{n(w_{2}, 3)}v_{w_{I}}\right)<br>\]</p>
<p>use property of sigmoid function that \(\sigma(x) + \sigma(-x) = 1\).</p>
<h4 id="2-3-Negative-Sampling"><a href="#2-3-Negative-Sampling" class="headerlink" title="2.3 Negative Sampling"></a>2.3 Negative Sampling</h4><p>An alternative to hierarchical softmax is Noise Contrastive Estimation (NCE). NCE can be shown to approximately maximize log probability of softmax, the skip-gram model is only concerned with learning high quality word vectors, so it’s free to simplify NCE as long as vector representation retain the quality. The task is to distinguish target word \(w_{O}\) from draws from noise distribution \(P_{n}(w)\) use logistic regression, where there are \(k\) negative samples for each positive sample.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>, <a href="/categories/Machine-Learning/Deep-Learning/">Deep Learning</a>, <a href="/categories/Machine-Learning/Deep-Learning/NLP/">NLP</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/NLP/">NLP</a><a href="/tags/Word-Embeddings/">Word Embeddings</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>