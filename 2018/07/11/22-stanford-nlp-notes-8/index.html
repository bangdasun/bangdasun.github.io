<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Stanford NLP (coursera) Notes (8) - Max Entropy Model | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Machine Learning,NLP,Maximum Entropy Model">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Stanford NLP (coursera) Notes (8) - Max Entropy Model"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/categories">Categories</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Stanford NLP (coursera) Notes (8) - Max Entropy Model</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/07/11/22-stanford-nlp-notes-8/" rel="bookmark">
        <time class="entry-date published" datetime="2018-07-12T02:52:20.000Z">
          2018-07-11
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Maximum Entropy Classifiers.</p>
<a id="more"></a>
<h3 id="1-Generative-vs-Discriminative"><a href="#1-Generative-vs-Discriminative" class="headerlink" title="1. Generative vs Discriminative"></a>1. Generative vs Discriminative</h3><p>So far we’ve looked at <strong>generative models</strong>, they are modeling <strong>joint distributions</strong>: where we have some data \((x, y)\) of paired observations and hidden classes \(y\), it place probabilities over both observed data and the hidden stuff - generate the observed data from hidden stuff, it try to maximize \(P(x, y)\). It turns out to be trivial to choose weights - just use relative frequencies. All classic statistical NLP models are generative models: N-gram models, Naive Bayes Classifier, Hidden Markov Models, Probabilistic Context-free grammars, etc.</p>
<p>But there is now much use of conditional or <strong>discriminative models</strong>, because</p>
<ul>
<li>they give high accuracy performance</li>
<li>they make it easy to incorporate lots of linguistically important features</li>
<li>they allow automatic building of language independent, retargetable NLP modules</li>
</ul>
<p>It models <strong>conditional distributions</strong>, take the data as given and put a probability over hidden structure given the data, it try to maximize \(P(y|x)\), includes: Logistic Regression, Conditional Loglinear / Maximum Entropy Models, Conditional Random Fields, SVM, Perceptron, etc. </p>
<h3 id="2-Extract-Features-from-Text"><a href="#2-Extract-Features-from-Text" class="headerlink" title="2. Extract Features from Text"></a>2. Extract Features from Text</h3><p>Features are elementary pieces of evidence that link aspects what we observed with a class \(y\) that we want to predict. It’s a function with a bounded real value:</p>
<p>\[<br>f: y\times x \rightarrow \mathbb{R}.<br>\]</p>
<p>In NLP uses, a features is an <strong>indicator function</strong> of properties of the input and a particular class. Example features: we want to classify if the document is LOCATION or DRUG, we can extract features like:</p>
<p>\[<br>\begin{aligned}<br>f_{1} &amp;= [y = \text{ LOCATION } \text{and }w_{-1}=\text{ “in” }\text{ and }isCapitalized(w)] \\<br>f_{2} &amp;= [y = \text{ LOCATION } \text{and }hasAccentedLatinChar(w)]<br>\end{aligned}<br>\]</p>
<p>Then when we train the model, it will assign to each feature a weight: a positive weight votes that this configuration is likely correct; a negative weight votes that this configuration is likely incorrect.</p>
<p>For <strong>feature expectations</strong>, we will make use two:</p>
<ul>
<li>empirical count (expectation) of a feature:</li>
</ul>
<p>\[<br>E(f_{i}) = \sum_{(x,y)}f_{i}(x,y).<br>\]</p>
<ul>
<li>model expectation of a feature:</li>
</ul>
<p>\[<br>E(f_{i}) = \sum_{(x,y)}p(x,y)f_{i}(x,y).<br>\]</p>
<h3 id="3-Linear-Classifiers"><a href="#3-Linear-Classifiers" class="headerlink" title="3. Linear Classifiers"></a>3. Linear Classifiers</h3><p>Key components:</p>
<ul>
<li>linear function from feature sets to classes</li>
<li>assign a weight \(w_i\) to each feature \(f_i\)</li>
<li>we consider each class for an observed data \(x\)</li>
<li>for a pair of data \((x, y)\), features vote with their weights:</li>
</ul>
<p>\[<br>Vote(y) = \sum \lambda_i f_i(x,y)<br>\]</p>
<ul>
<li>choose the class \(y\) which maximizes the vote</li>
</ul>
<p>There are many ways to choose weights, depends on the way we modeling and the way we train the model. Here we use <strong>Maximum Entropy Classifier</strong> (<strong>it is essentially multiclass logistic regression</strong>, in NLP it usually called Maximum Entropy Model), it model the conditional probability as follows:</p>
<p>\[<br>P(y|x) = \frac{\exp\left(\sum_i \lambda_i f_i(x,y)\right)}{\sum_y \exp\left(\sum_i \lambda_i f_i(x,y)\right)}.<br>\]</p>
<p>Here exponential function will make everything positive, and the denominator could be view as normalized votes. We can see the weights are the parameters of this model, combine via the <strong>softmax function</strong>.</p>
<p>The way we estimate the parameters is through maximizing the likelihood (MLE). Here it is, (we take a log)</p>
<p>\[<br>\log\prod_{(x,y)}P(y|x) = \sum_{(x,y)}\log P(y|x) = \sum_{(x,y)}\log \frac{\exp\left(\sum_i \lambda_i f_i(x,y)\right)}{\sum_y \exp\left(\sum_i \lambda_i f_i(x,y)\right)}<br>\]</p>
<h3 id="4-Maximum-Entropy-Models"><a href="#4-Maximum-Entropy-Models" class="headerlink" title="4. Maximum Entropy Models"></a>4. Maximum Entropy Models</h3><p>An equivalent approach: we want a distribution which is uniform except in specific ways we require. Uniformity means <strong>high entropy</strong>.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/NLP/">NLP</a><a href="/tags/Maximum-Entropy-Model/">Maximum Entropy Model</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2021 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>