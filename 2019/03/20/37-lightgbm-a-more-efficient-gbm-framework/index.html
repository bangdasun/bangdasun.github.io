<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>LightGBM - A more Efficient GBM Framework | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Ensemble Algorithms,Decision Tree,Machine Learning">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="LightGBM - A more Efficient GBM Framework"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>LightGBM - A more Efficient GBM Framework</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/03/20/37-lightgbm-a-more-efficient-gbm-framework/" rel="bookmark">
        <time class="entry-date published" datetime="2019-03-20T23:23:44.000Z">
          2019-03-20
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>[Paper reading] LightGBM: A Highly Efficient Gradient Boosting Decision Tree</p>
<a id="more"></a>
<p>About three years ago you can see many top solutions of kaggle competitions used XGBoost - just one year later Microsoft proposed <a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="external">LightGBM</a> - another GBM framework. And now if you go to kaggle, you can see LightGBM is gradually “replacing” XGBoost - both in top solutions and kernels. I’ve participated in about 6 competitions which require heavy feature engineerings + GBM, and I actually use XGBoost results in final submission for only once. Since LightGBM could always get same or even better performance than XGBoost in my experiences, and it runs much faster (single machine, CPU only). So this time I’ll go through its <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" target="_blank" rel="external">paper</a> to see what’s special.</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>As what we see in XGBoost paper, GBM is a powerful algorithm, but how to scale it to large and high dimensional data is a problem. Compared with vanilla GBM, XGBoost applies these techniques:</p>
<ul>
<li>better regularized loss function</li>
<li>reasonable approximation for loss function</li>
<li>shrinkage and subsampling (both sample and features)</li>
<li>optimized split finding algorithms</li>
<li>better lower-level system design</li>
</ul>
<p>Well, in LightGBM paper, the authors think XGBoost’s efficiency and scalability are still unsatisfactory when handling large and high dimensional data (LOL). And they proposed <strong>more optimized approaches in split finding</strong>: Gradient based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). They called this new framework LightGBM. And experiments show that LightGBM speeds up the training process of conventional GBM by up to over 20 times while achieving almost the same accuracy. More detailed experiments show that LightGBM can significantly outperform XGBoost in terms of computational speed and memory consumptions.</p>
<h3 id="2-Two-Important-Techniques"><a href="#2-Two-Important-Techniques" class="headerlink" title="2. Two Important Techniques"></a>2. Two Important Techniques</h3><p>The direction for optimization here are reducing sample size and feature dimensions, they could be achieved by subsampling - like Random Forest and XGBoost, they are also supported by LightGBM. Besides that, there are two novel techniques are used.</p>
<h4 id="2-1-Gradient-based-One-Side-Sampling-GOSS"><a href="#2-1-Gradient-based-One-Side-Sampling-GOSS" class="headerlink" title="2.1 Gradient based One-Side Sampling (GOSS)"></a>2.1 Gradient based One-Side Sampling (GOSS)</h4><p>The authors notice that samples with different gradients plays different roles in computing information gain. In particular, <strong>those samples with larger gradients will contribute more to the information gain</strong>. Therefore the idea to optimize is discard samples with small gradients - since they are already well trained.</p>
<p>However, this will change the distribution of the data, which might hurt the model performance. Here comes the <strong>GOSS</strong>.</p>
<p><strong>GOSS</strong> keeps all samples with large gradients and performs random sampling on samples with small gradients. In order to compensate the influence to the data distribution, when computing the information gain, GOSS introduces a constant multiplier for the samples with small gradients.</p>
<center><br>  <img src="/images/lgb_algo_2.PNG" style="width: 360px; height: 350px"> <br></center>

<p>In detail, GOSS firstly sorts the samples according to the absolute value of the gradients and select top \(a\)% samples, then randomly sample \(b\)% samples from rest of the data. After this, amplify sampled data with small gradients by a constant \((1-a)/b\) when calculating information gain. This way for under-trained samples, the original data distribution will not change too much.<br>The paper also posted theoreticall analysis of this approach, I won’t get into that details here.</p>
<h4 id="2-2-Exclusive-Feature-Bunding-EFB"><a href="#2-2-Exclusive-Feature-Bunding-EFB" class="headerlink" title="2.2 Exclusive Feature Bunding (EFB)"></a>2.2 Exclusive Feature Bunding (EFB)</h4><p>For sparse high dimensional data, it is possible to reduce the number features rather than subsampling. Specifically, in a sparse feature space, many features are mutually exlusive, which means they don’t take non-zero values at the same time (row). Then it is possible to <strong>bundle those features into a single feature</strong>. The key for this approach is to ensure that the values of original features can be identified from the bundled feature.<br>A simple example is: there are two features in a feature bundle, originally feature 1 takes values from \([0, 10)\) and feature 2 takes values from \([0, 20)\). Then an offset of 10 could be added to feature 2 then it will take values from \([10, 30)\). After this it is safe to merge these two features. Detailed algorithm is as this:</p>
<center><br>  <img src="/images/lgb_algo_4.PNG" style="width: 360px; height: 300px"> <br></center>




      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Ensemble-Algorithms/">Ensemble Algorithms</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/Machine-Learning/">Machine Learning</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>