<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>XGBoost - How it works | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Ensemble Algorithms,Decision Tree,Machine Learning">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="XGBoost - How it works"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>XGBoost - How it works</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/03/19/36-xgboost-how-it-works/" rel="bookmark">
        <time class="entry-date published" datetime="2019-03-19T23:19:56.000Z">
          2019-03-19
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>[Paper reading] XGBoost: A scalable Tree Boosting System</p>
<a id="more"></a>
<p>XGBoost is a open source software library which provides a gradient boosting algorithm framework. I won’t tell you how powerful it is since I cannot finish it in this small space, but the important factor behind its success is its scalability in all scenarios. It won John Chambers Award and High Energy Physics meets Machine Learning award in 2016. Here is the <a href="https://github.com/dmlc/xgboost" target="_blank" rel="external">github repo</a>, <a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="external">documents</a> and <a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf" target="_blank" rel="external">published paper</a>.</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Gradient tree boosting, a.k.a. gradient boosting machine (GBM) is a powerful algorithm in many applications. This paper describe XGBoost with innovations:</p>
<ul>
<li>a novel tree learning algorithm for handling sparse data</li>
<li>a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning</li>
<li>an efficient cache-aware block structure for out-of-core tree learning</li>
</ul>
<h3 id="2-Tree-Boosting-in-a-Nutshell"><a href="#2-Tree-Boosting-in-a-Nutshell" class="headerlink" title="2. Tree Boosting in a Nutshell"></a>2. Tree Boosting in a Nutshell</h3><h4 id="2-1-Regularized-Learning-Objective"><a href="#2-1-Regularized-Learning-Objective" class="headerlink" title="2.1 Regularized Learning Objective"></a>2.1 Regularized Learning Objective</h4><p>With input data \(\left\{(X_{i}, y_{i}), ~i=1,2,\cdots,n; ~X_{i}\in R^{m}, y_{i}\in R\right\}\), a tree ensemble model with \(K\) additive functions to predict the output<br>\[<br>\hat{y}_{i} = \sum^{K}_{k=1}f_{k}\left(X_{i}\right),<br>\]<br>where \(f_{k}\) is in the space of decision trees \(\mathcal{F}\), where \(\mathcal{F} = \left\{f(x) = w_{q(x)}\right\}\), \(q: R^{m}\rightarrow T\) and \(w\in R^{T}\). \(q\) denotes structure of the decision tree that maps a input \(x\) to leaf node. \(T\) is the number of leaves. Each tree contains a score on each leaf, \(w_{i}\) is the score of \(i\) th leaf, i.e. for a specific sample, it will go through the decision tree and get to a leaf with a final prediction \(w_{i}\)..<br>The \(f_{k}\) is from optimizing the objective function:<br>\[<br>\mathcal{L} = \sum^{n}_{i=1}l\left(\hat{y}_{i}, y_{i}\right) + \sum_{k}\Omega\left(f_{k}\right),<br>\]<br>where \(l\) is a differentiable loss function, and \(\Omega\) plays as regularization term,<br>\[<br>\Omega\left(f_{k}\right) = \gamma T + \frac{1}{2}\lambda ||w||^{2}.<br>\]</p>
<h4 id="2-2-Gradient-Tree-Boosting"><a href="#2-2-Gradient-Tree-Boosting" class="headerlink" title="2.2 Gradient Tree Boosting"></a>2.2 Gradient Tree Boosting</h4><ul>
<li>Setup loss function</li>
</ul>
<p><strong>Training GBM is sequentially</strong>. Let \(\hat{y}_{i}^{(t)}\) be the prediction of input \(X_{i}\) at \(t\) th iteration, a decision tree is trained and add to the output model of \((t-1)\) th iteration, the objective function is:<br>\[<br>\mathcal{L}^{(t)} = \sum^{n}_{i=1}l\left(y_{i}, \hat{y}_{i}^{(t-1)} + f_{t}\left(X_{i}\right)\right) + \Omega\left(f_{t}\right) = \sum^{n}_{i=1}l\left(y_{i}, \hat{y}_{i}^{(t)} \right) + \Omega\left(f_{t}\right).<br>\]</p>
<p>When the loss function \(l\) is squared loss or exponential loss, the calculation is easy, take <strong>squared loss</strong> as an example:<br>\[<br>l\left(y_{i}, \hat{y}_{i}^{(t-1)} + f_{t}\left(X_{i}\right)\right) = \left(y_{i} -\hat{y}_{i}^{(t-1)} -f_{t}\left(X_{i}\right) \right)^{2} = \left(r_{i}^{(t-1)} -f_{t}\left(X_{i}\right) \right)^{2},<br>\]</p>
<p>where \(r_{i}^{(t-1)}\) is the training residual at iteration \((t-1)\), and you can observe that the goal at iteration \(t\) is to <strong>fit a decision tree to the residual</strong> from last iteration. Also it is easy to see</p>
<p>\[<br>-\frac{\partial l\left(y_{i}, \hat{y}_{i}^{(t-1)} \right)}{\partial \hat{y}_{i}^{(t-1)}} = y_{i} - \hat{y}_{i}^{(t)}  = r_{i}^{(t-1)},<br>\]</p>
<p>this means the residual is also the gradient of the loss function which is calculated during training (if use gradient descent based algorithms).<br>However, sometimes the loss functions are complicated, therefore residual is not equivalent to gradient. GBM can handle other types of loss functions, and the target it use is <strong>negative gradient</strong>, so every single step - the decision tree is fit on negative gradient, no more “residual” . In some contexts it is also called <strong>pseudo-residual</strong>, I guess we usually just use “residual” in regression problem, and it could say <strong>negative gradient</strong> here is the <strong>approximation of residual</strong>.<br>In XGBoost framework, they use <strong>second-order approximation</strong> for objective function:<br>\[<br>\begin{aligned}<br>\mathcal{L}^{(t)} =&amp;  \sum^{n}_{i=1} \left[ l\left(y_{i}, \hat{y}_{i}^{(t-1)} + f_{t}(X_{i})\right)\right] + \Omega\left(f_{t}\right) \\<br>\approx&amp; \sum^{n}_{i=1} \left[ l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right) + g_{i}f_{t}\left(X_{i}\right) + \frac{1}{2}h_{i}f^{2}_{t}\left(X_{i}\right) \right] + \Omega\left(f_{t}\right),\\<br>\end{aligned}<br>\]<br>where<br>\[<br>g_{i} = \frac{\partial l\left(y_{i}, \hat{y}_{i}^{(t-1)} \right)}{\partial \hat{y}_{i}^{(t-1)}}, ~h_{i} = \frac{\partial^{2} l\left(y_{i}, \hat{y}_{i}^{(t-1)} \right)}{\partial \left(\hat{y}_{i}^{(t-1)}\right)^{2}}<br>\]<br>are the 1st and 2nd order gradient statistics on the loss function. Removing the constant term,<br>\[<br>\mathcal{L}^{(t)} \approx \sum^{n}_{i=1} \left[ g_{i}f_{t}\left(X_{i}\right) + \frac{1}{2}h_{i}f^{2}_{t}\left(X_{i}\right) \right] + \Omega\left(f_{t}\right).<br>\]</p>
<p>Define \(I_{j} = \left\{i | q(x_{i}) = j\right\}\) as samples of leaf \(j\), then the objective function is </p>
<p>\[<br>\begin{aligned}<br>\mathcal{L}^{(t)} &amp;\approx \sum^{n}_{i=1} \left[ g_{i}f_{t}\left(X_{i}\right) + \frac{1}{2}h_{i}f^{2}_{t}\left(X_{i}\right) \right] + \gamma T + \frac{1}{2}\lambda\sum^{T}_{j=1}w_{j}^{2} \\<br>&amp; = \sum^{T}_{j=1}\left[ \left(\sum_{i\in I_{j}} g_{i}\right)w_{j} + \frac{1}{2}\left(\sum_{i\in I_{j}} h_{i} + \lambda\right)w_{j}^{2} \right] + \gamma T.<br>\end{aligned}<br>\]</p>
<p>This step shows that <strong>train</strong> \(f_{t}(X_{i})\) <strong>by minimizing loss over all samples</strong> is equivalent to <strong>heuristically find the best</strong> \(f_{t}(X_{i})\) <strong>at each step</strong> \(t\) - it shows single decision tree learning is part of the sequential learning in XGBoost (it should be true for general gradient boosting algorithms learning).</p>
<ul>
<li>Minimizing total loss &amp; Training single decision tree</li>
</ul>
<p>For a fixed structure decision tree \(q(x)\), the optimal weight of leaf \(j\) is<br>\[<br>w_{j} = -\frac{\sum_{i\in I_{j}} g_{i}}{\sum_{i\in I_{j}} h_{i} + \lambda}<br>\]<br>with corresponding optimal value<br>\[<br>\mathcal{L}^{(t)}(q) = -\frac{1}{2}\sum^{T}_{j=1} \frac{\left(\sum_{i\in I_{j}} g_{i}\right)^{2}}{\sum_{i\in I_{j}} h_{i} + \lambda} + \gamma T.<br>\]<br>This equation can be used as a scoring function to measure the quality of a tree structure, like impurity score.<br>Then the node are splited into left and right branches, suppose \(I_{L}\) and \(I_{R}\) are the samples of left and right child nodes, let \(I = I_{L}\cup I_{R}\), the loss reduction after split is<br>\[<br>\mathcal{L}_{\text{split}} = \frac{1}{2}\left[  \frac{\left(\sum_{i\in I_{L}} g_{i}\right)^{2}}{\sum_{i\in I_{L}} h_{i} + \lambda} +   \frac{\left(\sum_{i\in I_{R}} g_{i}\right)^{2}}{\sum_{i\in I_{R}} h_{i} + \lambda}  -   \frac{\left(\sum_{i\in I} g_{i}\right)^{2}}{\sum_{i\in I} h_{i} + \lambda} \right] - \gamma<br>\]</p>
<h4 id="2-3-Shrinkage-and-Subsampling"><a href="#2-3-Shrinkage-and-Subsampling" class="headerlink" title="2.3 Shrinkage and Subsampling"></a>2.3 Shrinkage and Subsampling</h4><p>Besides the approximation and regularized objective function, compared with regular GBM, XGBoost also apply two tricks to prevent overfitting:</p>
<ul>
<li>Shrinkage: <strong>multiply a weight (between 0 and 1) to each new added tree</strong> to reduce the influence of individual tree for further trees to improve the model, this weight is usually also interpreted as <strong>learning rate</strong>. Notice that this weight is added in the loss function rather than model form, the model is still </li>
</ul>
<p>\[<br>\hat{y}_{i} = \sum^{K}_{k=1}f_{k}\left(X_{i}\right),<br>\]</p>
<ul>
<li>Subsampling: this is same with Random Forest (<strong>bootstrap</strong>), with both samples and features get sampled when fitting a new decision tree.</li>
</ul>
<h3 id="3-Split-Finding-Algorithm"><a href="#3-Split-Finding-Algorithm" class="headerlink" title="3. Split Finding Algorithm"></a>3. Split Finding Algorithm</h3><p>The most important part in fitting a decision tree is to find a split: which feature to split and what value as split threshold.</p>
<h4 id="3-1-Basic-Extra-Greedy-Algorithm"><a href="#3-1-Basic-Extra-Greedy-Algorithm" class="headerlink" title="3.1 Basic Extra Greedy Algorithm"></a>3.1 Basic Extra Greedy Algorithm</h4><p>The dumpiest way to find the split is brute-force: enumerate over all the possible splits on all features. This is what <code>sklearn</code>, <code>R</code> do. Single machine version of XGBoost also support this algorithm.</p>
<center><br>  <img src="/images/xgb_algo_1.PNG" style="width: 500px; height: 350px"> <br></center>

<h4 id="3-2-Approximate-Algorithm"><a href="#3-2-Approximate-Algorithm" class="headerlink" title="3.2 Approximate Algorithm"></a>3.2 Approximate Algorithm</h4><p>To make it possible to let the large dataset entirely into memory during split finding, an approximate algorithm is proposed.</p>
<center>  <img src="/images/xgb_algo_2.PNG" style="width: 510px; height: 320px"> </center>

<p>To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution. The algorithm then maps the continuous features into buckets split by the candidate points, aggregate the statistics and find the best solution among proposals based on the aggregated statistics.</p>
<h4 id="3-3-Weighted-Quantile-Sketch"><a href="#3-3-Weighted-Quantile-Sketch" class="headerlink" title="3.3 Weighted Quantile Sketch"></a>3.3 Weighted Quantile Sketch</h4><p>This part related to the approximation algorithm. I’ll temporary skip this part. <strong>TO BE CONTINUED</strong>…    </p>
<h4 id="3-4-Sparsity-aware-Split-Finding"><a href="#3-4-Sparsity-aware-Split-Finding" class="headerlink" title="3.4 Sparsity-aware Split Finding"></a>3.4 Sparsity-aware Split Finding</h4><p>There are multiple possible causes for data sparsity:</p>
<ul>
<li>presence of missing data</li>
<li>frequent 0 entries in statistics</li>
<li>artifacts of feature engineering such as one-hot encoding</li>
</ul>
<p>XGBoost add a <strong>default direction</strong> for each tree node. <strong>When a feature value is missing, the sample is classified into the default direction</strong>. There are two choice for default direction, the optimal is learnt from data: choose the direction to minimize loss. The key improvement is to only visit the non-missing entries. The presented algorithm treats the non-presence as missing value and learns the best direction to handle missing values.</p>
<h3 id="4-System-Design"><a href="#4-System-Design" class="headerlink" title="4. System Design"></a>4. System Design</h3><p>This part is skipped here since I don’t have too much experience in system design :) But this part is very important for XGBoost.</p>
<h3 id="5-Experiments-and-Conclusions"><a href="#5-Experiments-and-Conclusions" class="headerlink" title="5. Experiments and Conclusions"></a>5. Experiments and Conclusions</h3><p>In the paper, for classification task on single machine, XGBoost is evaluated on Higgs-1M data. Both XGBoost and <code>sklearn</code> give bettern performance than <code>R</code>‘s GBM, while XGBoost runs more than 10x faster than <code>sklearn</code>.</p>
<p>In summary, the paper proposed XGBoost - a scalable tree boosting system. Highlights include:</p>
<ul>
<li>a novel sparsity aware algorithm for handling sparse data.</li>
<li>a theoretically justified weighted quantile sketch for approximate learning.</li>
<li>experiments show that cache access patterns, data compression and sharding are essential elements for building a scalable end-to-end system for tree boosting.</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Ensemble-Algorithms/">Ensemble Algorithms</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/Machine-Learning/">Machine Learning</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2020 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>