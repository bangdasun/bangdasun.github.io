<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Practical Comparison of XGBoost and LightGBM | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Ensemble Algorithms,Decision Tree,Machine Learning">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Practical Comparison of XGBoost and LightGBM"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Practical Comparison of XGBoost and LightGBM</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/03/21/38-practical-comparison-xgboost-lightgbm/" rel="bookmark">
        <time class="entry-date published" datetime="2019-03-22T00:53:17.000Z">
          2019-03-21
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>A practical comparison of XGBoost and LightGBM</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Last two posts are XGBoost and LightGBM paper readings, they are official descriptions of these two GBM frameworks. However many practical details are not mentioned or described very clearly. Also, there are some different features between them. Therefore this time I’ll give a summary of comparison which will start from their mechanism and practical usage.</p>
<h3 id="2-Common-Tricks"><a href="#2-Common-Tricks" class="headerlink" title="2. Common Tricks"></a>2. Common Tricks</h3><h4 id="2-1-Tree-Growing"><a href="#2-1-Tree-Growing" class="headerlink" title="2.1 Tree Growing"></a>2.1 Tree Growing</h4><p>Both XGBoost and LightGBM support <strong>Best-first</strong> Tree Growth, a.k.a. <strong>Leaf-wise</strong> Tree Growth. Many other GBM implementation use <strong>Depth-first</strong> Tree Growth, a.k.a. <strong>Depth-wise</strong> Tree Growth. Use the description from LightGBM doc:</p>
<center><br>  <img src="/images/leaf-wise-tree-growth.PNG" style="width: 600px; height: 200px"> <br></center>

<p>For leaf-wise method, it will choose the leaf with max loss reduce to grow, rather than finish the leaf growth in same level. With number of leaves fixed, leaf-wise method tend to achieve lower loss than depth-wise method. Also leaf-wise method can converge much faster, but it will also be more likely to overfit.</p>
<p>Here is the depth-wise tree growth.</p>
<center><br>  <img src="/images/depth-wise-tree-growth.PNG" style="width: 500px; height: 200px"> <br></center>

<p>Unlike LightGBM, XGBoost also support depth-wise. The parameter is <code>grow_policy</code> with default to be <code>depthwise</code>; to use leaf-wise, switch to <code>lossguide</code>.</p>
<p>For leaf-wise tree growth, the key parameters are:</p>
<ul>
<li>number of leaves:<ul>
<li>XGBoost: <code>max_leaves</code> (need to set <code>grow_policy=lossguide</code>, otherwise it is 0)</li>
<li>LightGBM: <code>num_leaves</code></li>
</ul>
</li>
<li>max depth:<ul>
<li>XGBoost: <code>max_depth</code> (can set to 0 when <code>grow_policy=lossguide</code> and <code>tree_method=hist</code>)</li>
<li>LightGBM: <code>max_depth</code> (set to -1 means no limit)</li>
</ul>
</li>
<li>min data required in leaf to split:<ul>
<li>XGBoost: <code>min_child_weight</code></li>
<li>LightGBM: <code>min_data_in_leaf</code></li>
</ul>
</li>
</ul>
<h4 id="2-2-Histogram-based-Split-Finding"><a href="#2-2-Histogram-based-Split-Finding" class="headerlink" title="2.2 Histogram-based Split Finding"></a>2.2 Histogram-based Split Finding</h4><p>Both XGBoost and LightGBM support <strong>histogram-based</strong> algorithm for split finding. As mentioned in XGBoost paper, the exact-greedy (brute-force) split find algorithm is time consuming: for current feature to search, need to sort feature values and iterate through. For faster training, histogram-based algorithm is used, which bucket continuous feature into discrete bins. This speeds up training and reduces memory usage.</p>
<p>LightGBM is using histogram-based algorithm. Related parameters are:</p>
<ul>
<li><code>max_bin</code>: max number of bins that feature values will be bucketed in.</li>
<li><code>min_data_in_bin</code>: minimal number of data inside one bin.</li>
<li><code>bin_construct_sample_cnt</code>: number of data that sampled to construct histogram bins.</li>
</ul>
<p>XGBoost has options to choose histogram-based algorithm, it is specified by <code>tree_method</code> with options:</p>
<ul>
<li><code>auto</code>: (default) use heuristic to choose the fastest method.</li>
<li><code>exact</code>: exact greedy algorithm.</li>
<li><code>approx</code>: approximate greedy algorithm using quantile sketch and gradient histogram.</li>
<li><code>hist</code>: fast histogram optimized approximate greedy algorithm, with this option enabled, <code>max_bin</code> (default 256) could be tuned</li>
</ul>
<h4 id="2-3-Missing-Values-Handling"><a href="#2-3-Missing-Values-Handling" class="headerlink" title="2.3 Missing Values Handling"></a>2.3 Missing Values Handling</h4><p>Both XGBoost and LightGBM could handle missing values in input data. </p>
<p>XGBoost supports missing values by default. As mentioned in the paper, the missing values will be hold at first, then the optimal directions are learning during training to get best performance. On the other hand, XGBoost accepts sparse feature format where only non-zero values are stored, this way the data non-presented  are treated as missing.</p>
<p>LightGBM has several parameters for missing values handling:</p>
<ul>
<li><code>use_missing</code>: default to be <code>true</code>.</li>
<li><code>zero_as_missing</code>: default to be <code>false</code>, which means only <code>np.nan</code>  is considered as missing.</li>
</ul>
<h3 id="3-Different-Tricks"><a href="#3-Different-Tricks" class="headerlink" title="3. Different Tricks"></a>3. Different Tricks</h3><h4 id="3-1-Categorical-Feature-Handling"><a href="#3-1-Categorical-Feature-Handling" class="headerlink" title="3.1 Categorical Feature Handling"></a>3.1 Categorical Feature Handling</h4><p><strong>XGBoost currently only supports numerical inputs</strong>, which means if the categorical features are encoded as integers, they are treated as ranked numerical features, this could introduce bias. Therefore <strong>one-hot encoding</strong> needs to be applied before feed into XGBoost.</p>
<p>LightGBM supports categorical input type, use <code>categorical_feature</code>, notice:</p>
<ul>
<li>only supports integer type, e.g. outputs from <code>sklearn.LabelEncoder</code>.</li>
<li>index starts from 0 and it doesn’t count the label column when passing type is integer.</li>
<li>all values should be less than <code>Int32.MaxValue</code> (214783647).</li>
<li>all negative values will be treated as missing value.</li>
</ul>
<p>In LightGBM, how to find best split for categorical features is briefly described: the optimal solution is to split on categorical feature by partitioning its categories into 2 subsets, if the feature has <code>k</code> categories, there are <code>2^(k-1)-1</code> possible partitions, the basic idea is to sort the categories according to training objective at each split; more specifically, it sorts the histogram according to its accumulated values (<code>sum_gradient / sum_hessian</code>) and then finds the best split on the sorted histogram.</p>
<p>And there are also some parameters to handle categorical features regularization.</p>
<h4 id="3-2-Boosters"><a href="#3-2-Boosters" class="headerlink" title="3.2 Boosters"></a>3.2 Boosters</h4><p>As mentioned in LightGBM paper, a novel technique called <strong>gradient based one-side sampling</strong> is used, it could be set by <code>boosting=goss</code> with <code>top_rate</code> (between 0 and 1, the retain ratio of large gradient data) and <code>other_rate</code> (between 0 and 1, the retain ratio of small gradient data) specified.</p>
<p>In XGBoost, there are also multiple options :<code>gbtree</code>, <code>gblinear</code>, <code>dart</code> for boosters (<code>booster</code>), with default to be <code>gbtree</code>.</p>
<h3 id="4-Other-Things-to-Notice"><a href="#4-Other-Things-to-Notice" class="headerlink" title="4. Other Things to Notice"></a>4. Other Things to Notice</h3><h4 id="4-1-Feature-Importance"><a href="#4-1-Feature-Importance" class="headerlink" title="4.1 Feature Importance"></a>4.1 Feature Importance</h4><p>Feature importance is a good to validate and explain the results.  </p>
<p>LightGBM returns feature importance by calling<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lgb_model.feature_importance(importance_type, iteration=None)</div></pre></td></tr></table></figure></p>
<p>the choice of <code>importance_type</code> means different measures of feature importance</p>
<ul>
<li><code>&quot;split&quot;</code>: the number of times the feature is used to split data across all trees</li>
<li><code>&quot;gain&quot;</code>: the total gain of the feature when it is used across all trees</li>
</ul>
<p>XGBoost returns feature importance by calling<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xgb_model.get_score(fmap, importance_type)</div></pre></td></tr></table></figure></p>
<p>the choice of <code>importance_type</code> includes:</p>
<ul>
<li><code>&quot;weight&quot;</code>: the number of times the feature is used to split data across all trees</li>
<li><code>&quot;gain&quot;</code>: the average gain of the feature when it is used across all trees</li>
<li><code>&quot;cover&quot;</code>: the average coverage across all splits the feature is used (relative number of observations related to this feature)</li>
<li><code>&quot;total_gain&quot;</code></li>
<li><code>&quot;total_cover&quot;</code></li>
</ul>
<h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><p>There are still other topics worth discussing for XGBoost and LightGBM, e.g. they also both support <a href="https://arxiv.org/abs/1505.01866" target="_blank" rel="external">DART</a> boosting, which could have better performance but the parameter tuning is tricky; e.g. they could not only used on regression and classification tasks, but also <strong>learning to rank</strong> task. Anyway I’ll update here if there are other interesting comparisons could be done.</p>
<p>Here I also attach two examples of parameters usage for LightGBM and XGBoost.</p>
<ul>
<li>LightGBM</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">lgb_params = &#123;</div><div class="line">    &apos;objective&apos;         : &apos;regression&apos;,</div><div class="line">    &apos;boosting_type&apos;     : &apos;gbdt&apos;,</div><div class="line">    &apos;metric&apos;            : &apos;rmse&apos;,</div><div class="line">    &apos;learning_rate&apos;     : 0.01,</div><div class="line">    &apos;max_depth&apos;         : 8,</div><div class="line">    &apos;num_leaves&apos;        : 120,</div><div class="line">    &apos;min_data_in_leaf&apos;  : 90,</div><div class="line">    &apos;feature_fraction&apos;  : 0.185,</div><div class="line">    &apos;bagging_fraction&apos;  : 1,</div><div class="line">    &apos;data_random_seed&apos;  : 42,</div><div class="line">    &apos;lambda_l1&apos;         : 0.4,</div><div class="line">    &apos;lambda_l2&apos;         : 0.4,</div><div class="line">    &apos;cat_l2&apos;            : 15,</div><div class="line">    &apos;min_gain_to_split&apos; : 0.00,</div><div class="line">    &apos;min_data_per_group&apos;: 100,</div><div class="line">    &apos;max_bin&apos;           : 255,</div><div class="line">    &apos;nthread&apos;           : 4</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>XGBoost</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">xgb_params = &#123;</div><div class="line">    &apos;objective&apos;       : &apos;binary:logistic&apos;,</div><div class="line">    &apos;eval_metric&apos;     : &apos;auc&apos;,</div><div class="line">    &apos;eta&apos;             : 0.01,</div><div class="line">    &apos;max_depth&apos;       : 6,</div><div class="line">    &apos;min_child_weight&apos;: 3,</div><div class="line">    &apos;colsample_bytree&apos;: 0.151,</div><div class="line">    &apos;subsample&apos;       : 0.9,</div><div class="line">    &apos;gamma&apos;           : 0.00, # min_split_loss</div><div class="line">    &apos;max_delta_step&apos;  : 0.00,</div><div class="line">    &apos;lambda&apos;          : 1,    # L2 regularization</div><div class="line">    &apos;alpha&apos;           : 0,    # L1 regularization</div><div class="line">    &apos;scale_pos_weight&apos;: 1,    # control the balance of positive samples weights</div><div class="line">    &apos;seed&apos;            : 42</div><div class="line">&#125;</div></pre></td></tr></table></figure>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Ensemble-Algorithms/">Ensemble Algorithms</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/Machine-Learning/">Machine Learning</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>