<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (7) - LSTM and GRU | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Deep Learning,RNN">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (7) - LSTM and GRU"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (7) - LSTM and GRU</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/03/16/35-ml-overview-7-lstm-gru/" rel="bookmark">
        <time class="entry-date published" datetime="2019-03-16T15:13:09.000Z">
          2019-03-16
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Two important RNN structures</p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>It was one of the lectures of ECBM 4040 Deep Learning when I first knew RNN. I’ve used RNN like LSTM and GRU many times: course projects, kaggle competitions and work projects. They are both fundamental model structures for tasks require RNN or Sequence-to-Sequence (Seq2Seq) models, therefore this time I decide to briefly go over these two important RNN structures.</p>
<p>At the beginning I’d like to say thanks to <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">colah’s blog</a>, the style and structure are pretty clear, and the figures I use here are all from there.</p>
<h3 id="2-RNN-Recurrent-Neural-Networks"><a href="#2-RNN-Recurrent-Neural-Networks" class="headerlink" title="2. RNN (Recurrent Neural Networks)"></a>2. RNN (Recurrent Neural Networks)</h3><p>To deal with sequence data (time series, natural language), researchers proposed RNN which has a “time axis” in the network structure,</p>
<center><br>  <img src="/images/rnn.PNG" style="width: 500px; height: 150px"> <br></center>

<p>there is a input \(x\) for each timestamp, then it goes to hidden layers \(A\) and outputs \(h\). Then number of timestamps to use is a parameter, usually called <code>max_length</code>. For example data is a collection of sentences, then each word (timestamp) is a input \(x\), sentences with length smaller than <code>max_length</code> will be padded some number (e.g. 0) and sentences with length larger than <code>max_length</code> will be truncated, therefore finally all samples have same length of input.</p>
<p>A simple feedfoward step will be </p>
<p>\[<br>a_{t} = \tanh\left(W\left[a_{t-1}, x_{t}\right] + b\right),<br>\]</p>
<p>here \(a_{t}\) is the output of current hidden layers, \(a_{t-1}\) is the output of previous one hidden layers , \(\tanh()\) is the activation function (which could have many options), \(x_{t}\) is the input at current timestamp; \(W\) and \(b\) are trainable parameters.</p>
<p>But vanilla RNN performs bad on long inputs which require more information in the past to be extracted, in other words it is not able to capture “long-term dependecies”.</p>
<h3 id="3-Gradient-Vanishing"><a href="#3-Gradient-Vanishing" class="headerlink" title="3. Gradient Vanishing"></a>3. Gradient Vanishing</h3><p>The issue of losing long-term dependecies is from <strong>gradient vanishing</strong>. This is a common problem for back-propagation in deep neural networks, the gradients gradually vanish when it back propagate to earlier timestamps. For strict mathematical reasoning you could refer to <a href="https://tensorflowkorea.files.wordpress.com/2017/03/cs224n-2017winter-notes-all.pdf" target="_blank" rel="external">CS224n: Natural Language Processing with Deep Learning Lecture Notes</a> at part V. Another issue in training neural networks is <strong>gradient explosion</strong> which could be solved using gradient clipping. However, grading vanishing is harder to solve. But one of the solutions is using LSTM.</p>
<h3 id="4-LSTM"><a href="#4-LSTM" class="headerlink" title="4. LSTM"></a>4. LSTM</h3><p>The LSTM is proposed to avoid the gradient vanishing problem to capture long-term dependency.</p>
<center><br>  <img src="/images/lstm-0.PNG" style="width: 600px; height: 300px"> <br></center>

<p>Compared with vanilla RNN, LSTM has an extra component called <strong>cell</strong> state \(C_{t}\), along with regular hidden states \(h_{t}\).</p>
<center><br>  <img src="/images/lstm-1.PNG" style="width: 600px; height: 250px"> <br></center>

<p>Information will be either add to or remove from the cell state, it is controlled by the <strong>gates</strong>. LSTM contains three gates: <strong>forget gate</strong>, <strong>update gate</strong> and <strong>output gate</strong>.</p>
<h4 id="4-1-Forget-gate"><a href="#4-1-Forget-gate" class="headerlink" title="4.1 Forget gate"></a>4.1 Forget gate</h4><p>First step is go through forget gate. </p>
<center><br>  <img src="/images/lstm-2.PNG" style="width: 350px; height: 240px"> <br></center>

<p>it is a simple layer with sigmoid activation:</p>
<p>\[<br>f_{t} = \sigma\left(W_{f}\left[h_{t-1}, x_{t}\right] + b_{f}\right)<br>\]</p>
<p>\(f_{t}\) will be a value between 0 and 1, indicate the proportion to “keep” the information, therefore larger \(f_{t}\) means “forget” less.</p>
<h4 id="4-2-Update-gate"><a href="#4-2-Update-gate" class="headerlink" title="4.2 Update gate"></a>4.2 Update gate</h4><p>Second step is go through update gate.</p>
<center><br>  <img src="/images/lstm-3.PNG" style="width: 350px; height: 240px"> <br></center>

<p>it contains two steps:</p>
<ul>
<li>a sigmoid layer to indicate the proportion to update</li>
<li>a \(\tanh\) layer to create new value</li>
</ul>
<p>therefore:</p>
<p>\[<br>i_{t} = \sigma\left(W_{i}\left[h_{t-1}, x_{t}\right] + b_{i}\right) \\<br>\tilde{C_{t}} = \tanh\left(W_{C}\left[h_{t-1}, x_{t}\right] + b_{C}\right)<br>\]</p>
<p>\(i_{t}\) is a value between 0 and 1 to apply on \(\tilde{C_{t}}\), then the value will be added into cell state,</p>
<p>\[<br>C_{t} = f_{t}C_{t-1} + i_{t}\tilde{C_{t}}.<br>\]</p>
<h4 id="4-3-Output-gate"><a href="#4-3-Output-gate" class="headerlink" title="4.3 Output gate"></a>4.3 Output gate</h4><p>Finally it’s the output gate.</p>
<center><br>  <img src="/images/lstm-4.PNG" style="width: 350px; height: 240px"> <br></center>

<p>with,</p>
<p>\[<br>o_{t} = \sigma\left(W_{o}\left[h_{t-1}, x_{t}\right] + b_{o}\right) \\<br>h_{t} = o_{t}\tanh(C_{t})<br>\]</p>
<p>therefore \(o_{t}\) determines the proportion to output (hidden state).</p>
<h3 id="5-GRU"><a href="#5-GRU" class="headerlink" title="5. GRU"></a>5. GRU</h3><p>GRU (Gated Recurrent Unit) is a simplification of LSTM. It combines the forget gate and update gate in LSTM to a single update gate and it also combines cell state and hidden state.</p>
<center><br>  <img src="/images/gru.PNG" style="width: 350px; height: 240px"> <br></center>

<p>As a simplification version, GRU is supposed to perform better on less data and behave like what other simplification version models do on specific cases. <strong>TO BE CONTINUED</strong>…</p>
<h3 id="6-Summary"><a href="#6-Summary" class="headerlink" title="6. Summary"></a>6. Summary</h3><p>There are several nice papers to show why LSTM could avoid gradient vanishing problem. I’ll read them and make some notes for sure. In short, LSTM introduces gates to control, which is different from classical DNN vanilla RNN, this is the reason why gradient vanishing is prevented.</p>
<h3 id="7-References"><a href="#7-References" class="headerlink" title="7. References"></a>7. References</h3><ul>
<li>Colah’s blog,<ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
</ul>
</li>
<li>Stanford CS224n Notes,<ul>
<li><a href="https://tensorflowkorea.files.wordpress.com/2017/03/cs224n-2017winter-notes-all.pdf" target="_blank" rel="external">https://tensorflowkorea.files.wordpress.com/2017/03/cs224n-2017winter-notes-all.pdf</a></li>
</ul>
</li>
<li>Coursera Notes,<ul>
<li><a href="https://www.coursera.org/learn/nlp-sequence-models/home/" target="_blank" rel="external">https://www.coursera.org/learn/nlp-sequence-models/home/</a></li>
</ul>
</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>, <a href="/categories/Machine-Learning/Deep-Learning/">Deep Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/RNN/">RNN</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>