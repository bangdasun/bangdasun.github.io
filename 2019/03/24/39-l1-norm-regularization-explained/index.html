<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>L1 Norm Regularization Explained | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Mathematics,Machine Learning">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="L1 Norm Regularization Explained"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>L1 Norm Regularization Explained</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/03/24/39-l1-norm-regularization-explained/" rel="bookmark">
        <time class="entry-date published" datetime="2019-03-24T20:00:37.000Z">
          2019-03-24
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Why L1 norm regularization leads to sparsity solutions</p>
<a id="more"></a>
<p>The question was raised during this week’s Data Science Read Club of the company. And I found I cannot explain why L1 regularization encourage sparse results to myself, which is no good. Therefore this time I’ll briefly list some explanations from textbooks and websites.</p>
<h3 id="1-Geometric-Perspective"><a href="#1-Geometric-Perspective" class="headerlink" title="1. Geometric Perspective"></a>1. Geometric Perspective</h3><p>I think this interpretation is a “must-read” solution for every people who gets to the materials of comparison between <strong>Ridge</strong> and <strong>Lasso (Least Absolute Shrinkage and Selection Operator)</strong>, same for me.</p>
<p>From <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" target="_blank" rel="external">Trevor Hastie, et al The Elements of Statistical Learning</a> page 71:</p>
<center><br>  <img src="/images/l1-l2-norm.PNG" style="width: 500px; height: 300px"> <br></center>

<p>This is a example in 2D case, where the blue area represent the constraint of coefficients: on left: \(|\beta_{1}| + |\beta_{2}| \leq t\), the lasso regression; on right: \(\beta_{1}^{2} + \beta_{2}^{2}\leq t^{2}\), the ridge regression. And the red ellipses are the contours of loss functions. The solution will be the intersection of blue area and red contour. Well, the explanation is:</p>
<blockquote>
<p>“The sparsity of L1 regularization is because the solution occurs at corners, then the corresponding coefficients will equal to 0. For high dimensional case there are more corners and there are more opportunities for the coefficients to be 0”</p>
</blockquote>
<p>But for me it’s actually not very convincing. Why L2 regularization cannot hit at the corner?</p>
<h3 id="2-Analytical-Perspective"><a href="#2-Analytical-Perspective" class="headerlink" title="2. Analytical Perspective"></a>2. Analytical Perspective</h3><p>From answer in <a href="https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models" target="_blank" rel="external">https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models</a>, this answer has the highest upvotes.</p>
<p>Consider vector \(x = [1, \epsilon]\) where \(\epsilon &gt; 0\) is a small number. The L1 and L2 norm of \(x\) are given by:</p>
<p>\[<br>||x||_{1} = 1+\epsilon, ||x||_{2} = 1+\epsilon^{2}.<br>\]</p>
<p>Then apply a shrinkage on one component:</p>
<ul>
<li>Reduce 1 by \(\delta\) and \(\delta &lt; \epsilon\)</li>
</ul>
<p>The L1 and L2 norm become</p>
<p>\[<br>||x||_{1} = 1-\delta+\epsilon, ||x||_{2} = 1-2\delta+\delta^{2}+\epsilon^{2}.<br>\]</p>
<ul>
<li>Reduce \(\epsilon\) by \(\delta\) and \(\delta &lt; \epsilon\)</li>
</ul>
<p>The L1 and L2 norm become</p>
<p>\[<br>||x||_{1} = 1+\epsilon-\delta, ||x||_{2} = 1+\epsilon^{2}-2\epsilon\delta+\delta^{2}.<br>\]</p>
<p>Therefore for L1 norm, the shrinkage reduction is same for each coefficient, it move all coefficients towards 0 with same step size. And for L2 norm, regularizing large term results in a much greater reduction in norm than doing so to the smaller term.</p>
<h3 id="3-A-Simple-Example"><a href="#3-A-Simple-Example" class="headerlink" title="3. A Simple Example"></a>3. A Simple Example</h3><p>This example is from the Columbia STAT5241 ML notes.</p>
<p>Suppose \(n=p\), where \(n\) is sample size and \(p\) is feature dimension, input feature matrix is \(X=I\). </p>
<p>For ridge regression, the loss function is </p>
<p>\[<br>\sum^{p}_{j=1}\left(y_{j} - \beta_{j}\right)^{2} + \lambda\sum^{p}_{j=1}\beta_{j}^{2},<br>\]</p>
<p>and the solution is </p>
<p>\[<br>\hat{\beta}_{j} = \frac{y_{j}}{1+\lambda}.<br>\]</p>
<p>For lasso regression, the loss function is </p>
<p>\[<br>\sum^{p}_{j=1}\left(y_{j} - \beta_{j}\right)^{2} + \lambda\sum^{p}_{j=1}|\beta_{j}|,<br>\]</p>
<p>and the solution is depend on the sign of \(\beta\) since absolute value function is not differentiable at origin point.</p>
<ul>
<li>If \(\beta_{j} &gt; 0\):</li>
</ul>
<p>\[<br>\hat{\beta}_{j} = y_{j} - \frac{\lambda}{2}.<br>\]</p>
<ul>
<li>If \(\beta_{j} &lt; 0\):</li>
</ul>
<p>\[<br>\hat{\beta}_{j} = y_{j} + \frac{\lambda}{2}.<br>\]</p>
<p>In summary we can plot \(y_{j}\) vs \(\beta_{j}\):</p>
<center><br>  <img src="/images/beta_vs_y.PNG" style="width: 600px; height: 280px"> <br></center>

<p>Therefore when \(-\lambda/2 &lt; y_{j} &lt; \lambda/2\) we can see \(\beta_{j} = 0\) which introduces the sparsity.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Mathematics/">Mathematics</a><a href="/tags/Machine-Learning/">Machine Learning</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2019 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>