<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Overview Series (8) - Kmeans | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Clustering">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Machine Learning Overview Series (8) - Kmeans"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/archives">归档</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Machine Learning Overview Series (8) - Kmeans</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2020/10/11/71-ml-overview-8-kmeans/" rel="bookmark">
        <time class="entry-date published" datetime="2020-10-11T23:25:00.000Z">
          2020-10-11
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>K-means: a simple clustering algorithm based on distance. It is GMM (Gaussia Mixture Model) in probability perspective. </p>
<a id="more"></a>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>K-means can be quickly implemented from scratch:</p>
<blockquote>
<p>Step 1: Any data preprocessing (standardization, etc);<br>Step 2: Initialization,select k, randomly assign (hard assignment) cluster labels (from 1 to k) to each sample;<br>Step 3: While the cluster labels are not stabilized (converge): calculate the center of each cluster, re-assign cluster labels;<br>Step 4: Output results (cluster labels for each sample, cluster centers)</p>
</blockquote>
<p>Python code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize_data</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> (X - np.mean(X, axis=<span class="number">0</span>)) / np.std(X, axis=<span class="number">0</span>)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_distance</span><span class="params">(x, center)</span>:</span></div><div class="line">    <span class="comment"># x is a 1-d array</span></div><div class="line">    diff = x - center</div><div class="line">    <span class="keyword">return</span> diff.dot(diff.T)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_centers</span><span class="params">(X, cluster)</span>:</span></div><div class="line">    num_cluster = len(set(cluster))</div><div class="line">    num_features = X.shape[<span class="number">1</span>]</div><div class="line">    centers = np.zeros((num_cluster, num_features))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_cluster):</div><div class="line">        centers[i] = np.mean(X[cluster == i, :], axis=<span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> centers</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_cluster_label</span><span class="params">(x, centers)</span>:</span></div><div class="line">    num_cluster = centers.shape[<span class="number">0</span>]</div><div class="line">    distance = np.zeros(num_cluster)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_cluster):</div><div class="line">        distance[i] = calculate_distance(x, centers[i, :])</div><div class="line">    <span class="keyword">return</span> np.argmin(distance)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(X, k=<span class="number">3</span>, random_state=<span class="number">2020</span>)</span>:</span></div><div class="line">    np.random.seed(random_state)</div><div class="line">    </div><div class="line">    X_scaled = standardize_data(X)</div><div class="line">    num_obs, num_features = X_scaled.shape</div><div class="line">    centers = np.zeros((k, num_features))</div><div class="line">    assignment_matrix = np.zeros((num_obs, <span class="number">1</span>))</div><div class="line">    prev_clusters = assignment_matrix[:, <span class="number">0</span>]</div><div class="line">    curr_clusters = np.random.choice(np.arange(k), size=num_obs, replace=<span class="keyword">True</span>)</div><div class="line">    </div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> np.array_equal(prev_clusters, curr_clusters):</div><div class="line">        assignment_matrix = np.hstack((assignment_matrix, curr_clusters[:, np.newaxis]))</div><div class="line">        prev_clusters = curr_clusters.copy()</div><div class="line">        centers = calculate_centers(X_scaled, curr_clusters)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_obs):</div><div class="line">            curr_clusters[i] = assign_cluster_label(X_scaled[i, :], centers)</div><div class="line">        <span class="keyword">if</span> np.array_equal(prev_clusters, curr_clusters):</div><div class="line">            <span class="keyword">break</span></div><div class="line">        </div><div class="line">    <span class="keyword">return</span> curr_clusters + <span class="number">1</span>, center, assignment_matrix</div></pre></td></tr></table></figure>
<p>Kmeans clustering algorithm <strong>always converges</strong> to a local minimum of</p>
<p>\[<br>W(C) = \frac{1}{2}\sum^{K}_{k=1}\sum_{C(i) = k}\sum_{C(j) = k}d(x_{i}, x_{j}),<br>\]</p>
<p>intuitively it reduces the distances between samples within each cluster, but the actually results depend on the initialization of the algorithm.</p>
<p>Kmeans clustering is also interpreted as <strong>a finite mixture model with unit variance normal distributions</strong>. Mixture models can be described as two stage sampling procedures.</p>
<p>Something to notice:</p>
<ul>
<li>Euclidean distance can be replaced with other measures, for example correlation coefficient</li>
<li>An alternative: <strong>K-medoids</strong>, the centers are samples (useful when features are 0 or 1)</li>
</ul>
<h3 id="2-Selecting-K"><a href="#2-Selecting-K" class="headerlink" title="2. Selecting K"></a>2. Selecting K</h3><p>Selecting K is also know as <strong>model order selection</strong> in mixture model. </p>
<h4 id="2-1-Penalization-Method"><a href="#2-1-Penalization-Method" class="headerlink" title="2.1 Penalization Method"></a>2.1 Penalization Method</h4><p>The likelihood of the model is</p>
<p>\[<br>L(x;\theta) = \log\prod^{n}_{i=1}\left(\sum^{K}_{k=1}c_{k}p(x_{i}|\theta_{k})\right),<br>\]</p>
<p>and looks like we just need to select \(K\) which could maximize the likelihood , but <strong>this is not true</strong>: the optimal is achieved when each data point fit a cluster, this is overfit.</p>
<p>Therefore adding penalty term to the likelihood function could work. Two typical choices:</p>
<p>\[<br>\begin{aligned}<br>\phi_{\text{AIC}}(K) &amp;= K \\<br>\phi_{\text{BIC}}(K) &amp;= \frac{1}{2}K\log n<br>\end{aligned}<br>\]</p>
<p>As noted, they are equivalent to AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).</p>
<h4 id="2-2-Stability-Method"><a href="#2-2-Stability-Method" class="headerlink" title="2.2 Stability Method"></a>2.2 Stability Method</h4><p>Computing a stability score. Randomly split data into two parts (equal size), then for each data point fit for model one, compute its cluster label using model two. Intuitively it measures <strong>how many points are assigned to a different cluster under two models</strong>. \(K\) with minimum stability score is a better choice.</p>
<h4 id="2-3-Gap-Statistic-Method"><a href="#2-3-Gap-Statistic-Method" class="headerlink" title="2.3 Gap Statistic Method"></a>2.3 Gap Statistic Method</h4><p>Let \(D_{r}\) be the sum of all pairwise distances for all data points in cluster \(r\):</p>
<p>\[<br>D_{r} = \sum_{i, j\in C_{r}}d_{ij}.<br>\]</p>
<p>Set within-cluster mean distance \(W_{K}\):</p>
<p>\[<br>W_{K} = \sum^{K}_{r=1}\frac{1}{2n_{r}}D_{r}.<br>\]</p>
<p>The idea is to standardize the curve of \(\log W_{K}\) by comparing it with its expectation under an appropriate null reference distribution of the data. The optimal \(K\) makes \(\log W_{K}\) falls the farthest below the reference curve, therefore the Gap statistic is defined as</p>
<p>\[<br>Gap_{n}(K) = E_{n}\left[\log W_{k}\right] - \log W_{K}.<br>\]</p>
<h4 id="2-4-Elbow-Method"><a href="#2-4-Elbow-Method" class="headerlink" title="2.4 Elbow Method"></a>2.4 Elbow Method</h4><p>Compute the within-cluster sum of square (WSS), i.e. the square of the distances of data points to cluster center. Choose \(K\) where WSS first starts to diminish. It is ambiguous from my perspective.</p>
<h4 id="2-5-Silhouette-Method"><a href="#2-5-Silhouette-Method" class="headerlink" title="2.5 Silhouette Method"></a>2.5 Silhouette Method</h4><p>The Silhouette value measure how similar a point is to its own cluster compared to other clusters. Silhouette value is calculated using the mean within-cluster distance \(a\) and mean nearest-cluster distance \(b\) for each data point: \((b - a) / \max(a, b)\). \(K\) with highest Silhouette value is consider better choice.</p>
<h4 id="2-6-Summary"><a href="#2-6-Summary" class="headerlink" title="2.6 Summary"></a>2.6 Summary</h4><p>Penalization method is from mixture model perspective, other methods are based on distance measures, as distance is the key concept in clustering algorithms.</p>
<h3 id="3-References"><a href="#3-References" class="headerlink" title="3. References"></a>3. References</h3><ul>
<li>Determining The Optimal Number Of Clusters: 3 Must Know Methods<ul>
<li><a href="https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/#gap-statistic-method" target="_blank" rel="external">https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/#gap-statistic-method</a></li>
</ul>
</li>
<li>How to Determine the Optimal K for K-Means?<ul>
<li><a href="https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb" target="_blank" rel="external">https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb</a></li>
</ul>
</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Clustering/">Clustering</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2021 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>