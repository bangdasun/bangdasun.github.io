<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>The Beauty of Mathematics in Computer Science Notes | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Machine Learning,Mathematics,NLP">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="The Beauty of Mathematics in Computer Science Notes"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/archives">归档</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>The Beauty of Mathematics in Computer Science Notes</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2020/06/27/68-the-beauty-of-mathematics-in-computer-science-notes/" rel="bookmark">
        <time class="entry-date published" datetime="2020-06-27T03:25:34.000Z">
          2020-06-27
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Reading notes of “The Beauty of Mathematics in Computer Science” by Dr. Jun Wu. The book is mainly about mathematical applications in real world, including NLP, information theory, information retrieval.</p>
<a id="more"></a>
<p>I was a freshman (2012) when I first read this book, to be honest at that time this book didn’t impress me a lot - since I had no basis in this area. Now in 2020, as I’ve been working as a Data Scientist for two years, I read this book again, I’m really impressed and regret I didn’t deep dive into this area when I first read it.</p>
<p>This time I spend three days on reading it, here are some notes I highlighted.</p>
<h3 id="1-Natural-Language-Processing-From-Rule-Based-to-Statistical-Based"><a href="#1-Natural-Language-Processing-From-Rule-Based-to-Statistical-Based" class="headerlink" title="1. Natural Language Processing - From Rule Based to Statistical Based"></a>1. Natural Language Processing - From Rule Based to Statistical Based</h3><p>In 1960s, <strong>syntactic</strong> and <strong>semantic</strong> analysis were regarded as two basic dimensions in NLP. In syntactic analysis, as the grammar rules, part of speech, morphologic and etc can be abstracted into algorithms, rule based parsers were built to generate the parse / syntax tree for simple sentences. However it will generate exponential possible solutions for complex sentences in real world, it cannot scale up. Also, natural language cannot be described by <strong>context free grammar</strong>.</p>
<p>Statistical based parsers were developed started in 1970s. With the power of statistics, more successful NLP applications are developed:</p>
<ul>
<li>speech recognition</li>
<li>machine translation</li>
<li>syntactic analysis</li>
<li>…</li>
</ul>
<h3 id="2-Hidden-Markov-Model"><a href="#2-Hidden-Markov-Model" class="headerlink" title="2. Hidden Markov Model"></a>2. Hidden Markov Model</h3><p>A communication system with six basic elements in communication process (sender, ideas, encoding, communication channel, receiver, decoding):</p>
<center><br><img src="/images/communication-sys.PNG" ,="" style="width: 475px; height: 100px"> <br></center>

<p>The basic problem is how to inference the source information based on observations:</p>
<p>\[<br>s_{1}, s_{2}, s_{3}, \cdots = \arg\max_{s_{1}, s_{2}, s_{3}, \cdots} P(s_{1}, s_{2}, s_{3}, \cdots|o_{1}, o_{2}, o_{3}, \cdots).<br>\]</p>
<p>Using bayes formula:</p>
<p>\[<br>P(s_{1}, s_{2}, s_{3}, \cdots|o_{1}, o_{2}, o_{3}, \cdots) = \frac{P(o_{1},o_{2},o_{3},\cdots|s_{1}, s_{2}, s_{3},\cdots)P(s_{1},s_{2},s_{3},\cdots)}{P(o_{1},o_{2},o_{3},\cdots)}.<br>\]</p>
<p>Hidden Markov Model can be used then. Using Markov property:</p>
<p>\[<br>\begin{aligned}<br>P(o_{1}, o_{2}, o_{3},\cdots|s_{1}, s_{2}, s_{3},\cdots) &amp;= \prod_{t}P(o_{t}|s_{t}) \\<br>P(s_{1}, s_{2}, s_{3},\cdots)&amp;=\prod_{t}P(s_{t}|s_{t-1}).<br>\end{aligned}<br>\]</p>
<p>In NLP, \(P(s_{1}, s_{2}, s_{3},\cdots)\) is the language model. \(P(o_{1}, o_{2}, o_{3},\cdots|s_{1}, s_{2}, s_{3},\cdots)\) has different names in different areas, it is Acoustic Model in speech recognition; it is Translation Model in machine translation; it is Correction Model in spell correction.</p>
<p>Three basic use cases of Hidden Markov Model:</p>
<ul>
<li>given the model, calculate the probability of specific outputs (forward-backward algorithm)</li>
<li>given the model and observed outputs, find the most likely inputs (Viterbi algorithm)</li>
<li>estimate the model given sufficient observed data (Hidden Markov Model training)</li>
</ul>
<h3 id="3-Information-Content-and-TFIDF"><a href="#3-Information-Content-and-TFIDF" class="headerlink" title="3. Information Content and TFIDF"></a>3. Information Content and TFIDF</h3><p>The measure of information is called <strong>entropy</strong>:</p>
<p>\[<br>H(X) = -\sum_{x}P(x)\log P(x).<br>\]</p>
<p>In average, the entropy of chinese character is 8 - 9 bits (using 8 - 9 binary number). Consider the context dependent, every chinese character has around 5 bits, there a chinese book with 500 throusands characters has 2.5 million bits.</p>
<p>In web search, the weight of the keyword \(w\) in the query should reflect the informativeness. A simple approach is using the entropy,<br>\[<br>I(w) = -P(w)\log P(w) = -\frac{TF(w)}{N}\log\frac{TF(w)}{N} = \frac{TF(w)}{N}\log\frac{N}{TF(w)},<br>\]</p>
<p>where \(N\) is the size of the vocabulary, it can be ignored since it is a constant. Assume (1) the size of each document \(M\) are same,</p>
<p>\[<br>M = \frac{N}{D} = \frac{1}{D}\sum_{w}TF(w).<br>\]</p>
<p>(2) the contribution of keyword in the document is not relevant to the frequency, therefore a word can appear \(c(w) = TF(w) / D(w)\) times (\(c(w) &lt; M\)) or zero times, then </p>
<p>\[<br>TF(w)\log\frac{N}{TF(w)} = TF(w)\log\frac{MD}{c(w)D(w)} = TF(w)\log\left(\frac{D}{D(w)}\frac{M}{c(w)}\right).<br>\]</p>
<p>And  <strong>TFIDF</strong> is:</p>
<p>\[<br>TFIDF(w) = TF(w)\log\frac{D}{D(w)} = I(w) - TF(w)\log\frac{M}{c(w)}.<br>\]</p>
<p>Therefore the more informative \(w\) is, the larger TFIDF value is; the larger \(c(w)\) is (query keywords match word in documents), the larger TFIDF is. These two conclusions are consistent with the information theory.</p>
<h3 id="4-Text-Categorization-and-SVD"><a href="#4-Text-Categorization-and-SVD" class="headerlink" title="4. Text Categorization and SVD"></a>4. Text Categorization and SVD</h3><p>Two common text categorization (clustering) tasks: categorize documents by topics and categorize words by meanings. These two tasks can be done by calculating the cosine similarity of document / word vectors from <strong>word-document matrix</strong> \(X\).</p>
<p>When the matrix is very large, SVD can be applied:</p>
<p>\[<br>X = U\Sigma V^\top,<br>\]</p>
<p>with interpretations:</p>
<ul>
<li>matrix \(U\) is the result of words categrization, each row represents a word, each column represents a class with similar word meanings (semantic)</li>
<li>matrix \(V^{\top}\) is the result of document categorization, each row represents a document, each column represent a topic</li>
</ul>
<p>I’ve summarized the basic usage of SVD in python <a href="https://github.com/bangdasun/bangdasun.github.io/blob/master/posts/python_svd.ipynb" target="_blank" rel="external">here</a>.</p>
<h3 id="5-Maximum-Entropy-Model"><a href="#5-Maximum-Entropy-Model" class="headerlink" title="5. Maximum Entropy Model"></a>5. Maximum Entropy Model</h3><p><strong>Maximum Entropy principal</strong>: when make predictions on probability distributions for random events, no subject assumptions should be made - in this situation, the most uniform distribution has the lowest risk and highest entropy.</p>
<p>The maximum entropy model for next word prediction:</p>
<p>\[<br>P(w_{3}|w_{1}, w_{2}, s) = \frac{1}{Z(w_{1}, w_{2}, s)}e^{\lambda_{1}(w_{1}, w_{2}, w_{3})+\lambda_{2}(s, w_{3})},<br>\]</p>
<p>where \(Z\) is the normalization term.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/NLP/">NLP</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Mathematics/">Mathematics</a><a href="/tags/NLP/">NLP</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2021 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>