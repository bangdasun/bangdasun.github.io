<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Stanford NLP (Coursera) Notes (18) - Word Meaning and Similarity | Bangda Sun | Practice makes perfect</title>

  
  <meta name="author" content="Bangda">
  

  
  <meta name="description" content="Play with data">
  

  
  
  <meta name="keywords" content="Linguistic,NLP">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Stanford NLP (Coursera) Notes (18) - Word Meaning and Similarity"/>

  <meta property="og:site_name" content="Bangda Sun"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Bangda Sun" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX","output/HTML-CSS"],
    extensions: ["tex2jax.js","Safe.js"]
  });
  </script>
<script type="text/javascript" async="async"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML,Safe">
</script>
</script>
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Bangda Sun</a>
    </h1>
    <p class="site-description">Practice makes perfect</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Stanford NLP (Coursera) Notes (18) - Word Meaning and Similarity</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2020/06/25/65-stanford-nlp-notes-18/" rel="bookmark">
        <time class="entry-date published" datetime="2020-06-26T00:10:04.000Z">
          2020-06-25
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>Semantics</p>
<a id="more"></a>
<h3 id="1-Word-Senses-and-Word-Relations"><a href="#1-Word-Senses-and-Word-Relations" class="headerlink" title="1. Word Senses and Word Relations"></a>1. Word Senses and Word Relations</h3><p><strong>Lemma</strong> and <strong>Word Form</strong>: Lemma is same stem, POS, rough semantics, each lemma can have multiple meanings, or sense - defined as a discrete representation. For example, “bank” has two senses.</p>
<p><strong>Homonymy</strong>: words with same form but different meanings. It has two sub-categories: <strong>homograph</strong> - same form and <strong>homophone</strong> - different form but pronounce same.</p>
<p><strong>Polysemy</strong>: words have related meanings.</p>
<p><strong>Synonyms</strong> and <strong>Antonyms</strong>: Synonyms are words that have the same meaning in some or all contexts, the relation is between sense rather than words. For example, big and large have same meaning in some contexts, while in some other contexts they are not - we can only say “big sister” and not “large sister”. Antonyms are words that have the opposite meaning.</p>
<p><strong>Hyponymy</strong> and <strong>Hypernymy</strong>: Hyponymy in formal is <strong>IS-A hierarchy</strong> - A is a B, and B subsumes A.</p>
<h3 id="2-WordNet-and-other-Online-Thesauri"><a href="#2-WordNet-and-other-Online-Thesauri" class="headerlink" title="2. WordNet and other Online Thesauri"></a>2. WordNet and other Online Thesauri</h3><p>Applications of Thesauri and Ontologies (a model to describe the relationship between objects, their attributes and relationships):</p>
<ul>
<li>Information Extraction</li>
<li>Information Retrieval</li>
<li>Question Answering</li>
<li>Machine Translation</li>
<li>…</li>
</ul>
<p>The definition of “sense” in WordNet is the <strong>synset</strong>, i.e. synonym set. WordNet is available in <code>nltk</code>.</p>
<h3 id="3-Word-Similarity-Thesaurus-Method"><a href="#3-Word-Similarity-Thesaurus-Method" class="headerlink" title="3. Word Similarity: Thesaurus Method"></a>3. Word Similarity: Thesaurus Method</h3><p>Word similarity is not word relatedness. In one word, Thesaurus based methods calculate how words “nearby” in hypernym hierarchy.</p>
<p><strong>Path based similarity</strong>: denote \(pathlen(c_{1}, c_{2})\) as the number of edges in the shortest path between two nodes in hypernym graph. Then the similarity is defined as </p>
<p>\[<br>sim(c_{1}, c_{2}) = \max_{c_{1}\in\text{sense}(w_{1}), c_{2}\in\text{sense}(w_{2})} \frac{1}{pathlen(c_{1}, c_{2})}.<br>\]</p>
<p>The issue for this simple method is, all edges are equally weighted, but nodes high in the hierarchy are very abstract. Improved methods are proposed:</p>
<ul>
<li>represent the weight of each edge independently</li>
<li>words connected only through abstract nodes are less similar</li>
</ul>
<p><strong>Information content similarity (Resnik method)</strong>: define \(P(c)\) as the probability that a randomly selected word in a corpus is an instance of concept \(c\). Formally, there is a distinct random variable, ranging over words, associated with each concept in the hierarchy. The lower a node in the hierarchy, the lower its probability. Let \(word(c)\) be the set of all words that are children of node \(c\), then</p>
<p>\[<br>P(c) = \frac{1}{N}\sum_{w\in word(c)} count(w).<br>\]</p>
<p>The information content is defined as \(IC(c) = -\log P(c)\), and most informative subsumer \(LCS(c_{1}, c_{2})\) is defined as the most informative (lowest) node in the hierarchy subsuming both \(c_{1}\) and \(c_{2}\). The similarity between two words is related to their common information:</p>
<p>\[<br>sim(c_{1}, c_{2}) = -\log P(LCS(c_{1}, c_{2})).<br>\]</p>
<h3 id="4-Word-Similarity-Distributional-Method"><a href="#4-Word-Similarity-Distributional-Method" class="headerlink" title="4. Word Similarity: Distributional Method"></a>4. Word Similarity: Distributional Method</h3><p>Distributional based methods (a.k.a. Vector Space Models) calculate word similarities in terms of whether they have similar distributional contexts, as Thesaurus based methods have these issues:</p>
<ul>
<li>not every language has thesaurus</li>
<li>many words and phrases are missing</li>
<li>Thesauri work less well for verbs and adjectives as they have less structured hyponymy relations</li>
</ul>
<p>Recall term-document matrix from previous section, two words are defined as similar if their vector are similar. Unlike using the all counts when calculating document similarity, vectors for words can be based on a small context window, therefore a word is defined by a vector over counts of context words, therefore the <strong>term-context</strong> matrix is built. Also, unlike using raw counts or TFIDF in term-document matrix, <strong>Positive Pointwise Mutual Information (PPMI)</strong> is used. Pointwise mutual information between two words is</p>
<p>\[<br>PMI(w_{1}, w_{2}) = \log_{2}\frac{P(w_{1}, P(w_{2})}{P(w_{1})P(w_{2})},<br>\]</p>
<p>and PPMI is just replace all negative values with 0.</p>
<p>Cosine similarity can still be used:</p>
<p>\[<br>cosine(\mathbf{v},\mathbf{w}) = \frac{\sum^{N}_{i=1}v_{i}w_{i}}{\sqrt{\sum^{N}_{i=1}v_{i}^{2}\sum^{N}_{i=1}w_{i}^{2}}},<br>\]</p>
<p>where \(v_{i}\) is the PPMI for word \(v\) in context \(i\), \(w_{i}\) is the PPMI for word \(w\) in context \(i\).</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/NLP/">NLP</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Linguistic/">Linguistic</a><a href="/tags/NLP/">NLP</a>
    </span>
    

    </div>

    
  </div>
</article>

  
<section id="comment">
  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>



    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2020 Bangda
    
	
	
	
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  <span id="busuanzi_container_site_pv">
    Total viewer <span id="busuanzi_value_site_pv"></span>; 
  </span>
  
  <span id="busuanzi_container_site_uv">
    Total visitor <span id="busuanzi_value_site_uv"></span>
  </span>
	
	
  <div id="disqus_thread"></div>
  <script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
   */
   
  var disqus_config = function () {
	this.page.url = 'https://bangdasun.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = '{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://bangdasun-github-io.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
  <script id="dsq-count-scr" src="//bangdasun-github-io.disqus.com/count.js" async></script>
</footer>


    
  </div>
</div>
</body>
</html>